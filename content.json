{"meta":{"title":"Frank Wan's Blog","subtitle":"Learning & Studying","description":"Stay Hungry, Stay Foolish","author":"Frank Wan","url":"http://frankmartinem.github.io","root":"/"},"pages":[{"title":"书单","date":"2021-07-24T17:41:56.166Z","updated":"2021-07-24T17:41:56.166Z","comments":false,"path":"books/index.html","permalink":"http://frankmartinem.github.io/books/index.html","excerpt":"","text":"douban 豆瓣书单 douban: user: 122349831 # 豆瓣用户名 start: 0 # 从哪一条记录开始 count: 100 # 获取豆瓣书单数据条数"},{"title":"关于","date":"2021-07-24T16:22:19.510Z","updated":"2021-07-24T16:22:19.510Z","comments":false,"path":"about/index.html","permalink":"http://frankmartinem.github.io/about/index.html","excerpt":"","text":"姓名：Frank Wan地址：浙江大学玉泉校区邮箱：&#x66;&#x72;&#97;&#110;&#x6b;&#109;&#x61;&#x72;&#x74;&#105;&#x6e;&#101;&#x74;&#64;&#113;&#113;&#x2e;&#99;&#111;&#x6d;"},{"title":"分类","date":"2021-07-24T16:19:36.613Z","updated":"2021-07-24T16:19:36.613Z","comments":false,"path":"categories/index.html","permalink":"http://frankmartinem.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-07-24T16:19:36.650Z","updated":"2021-07-24T16:19:36.650Z","comments":true,"path":"links/index.html","permalink":"http://frankmartinem.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-07-24T16:19:36.690Z","updated":"2021-07-24T16:19:36.689Z","comments":false,"path":"repository/index.html","permalink":"http://frankmartinem.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-07-24T16:19:36.727Z","updated":"2021-07-24T16:19:36.727Z","comments":false,"path":"tags/index.html","permalink":"http://frankmartinem.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Qt Deploy","slug":"Qt-Deploy","date":"2021-07-06T03:48:57.000Z","updated":"2021-07-06T03:57:27.899Z","comments":true,"path":"2021/07/06/Qt-Deploy/","link":"","permalink":"http://frankmartinem.github.io/2021/07/06/Qt-Deploy/","excerpt":"","text":"发布带运行环境的Qt执行文件在VS中生成release版本的exe文件后，文件会依赖于使用的lib以及dll文件等。这样换一个运行环境后，可能会无法运行当前的文件。例如基于Qt的exe文件，会依赖于Qt的lib和dll。Qt提供了此问题的解决办法。 解决方法 打开Qt提供的cmd窗口 cd到release版本的exe所在的位置 运行 windeployqt file_name.exe 当前文件夹下的文件就是对应exe文件所需的运行环境上述操作生成的dll文件以及运行环境，能保证exe文件在没有安装qt环境的PC上运行。此方法只适用于windows系统（运行指令的意思就是win deploy qt的意思）。","categories":[{"name":"coding","slug":"coding","permalink":"http://frankmartinem.github.io/categories/coding/"}],"tags":[{"name":"software","slug":"software","permalink":"http://frankmartinem.github.io/tags/software/"}]},{"title":"Difference in Debug and Release","slug":"Difference-in-Debug-and-Release","date":"2021-07-06T02:43:13.000Z","updated":"2021-07-06T03:46:32.709Z","comments":true,"path":"2021/07/06/Difference-in-Debug-and-Release/","link":"","permalink":"http://frankmartinem.github.io/2021/07/06/Difference-in-Debug-and-Release/","excerpt":"","text":"VS中Debug和Release的区别问题1问题描述在搭建EMG信号处理系统时，我需要读取一个txt文件。在debug时能正常读取。在release版本下却不能获取其中的信息。后来发现问题在于我使用了assert语句。我的读取代码如下： 12345678910111213141516void Config::gen_data_list()&#123; QFile f(select_dataset_path); assert(f.open(QIODevice::ReadOnly | QIODevice::Text)); while (!f.atEnd()) &#123; QString l &#x3D; f.readLine(); qDebug() &lt;&lt; l &lt;&lt; endl; if (l.compare(&#39;\\n&#39;) &#x3D;&#x3D; 0) continue; else data_list.append(l.simplified()); &#125; data_num &#x3D; data_list.length(); trial_num &#x3D; data_num * blk_num;&#125; 解决办法在release条件下，data_num始终是0。因为assert语句被忽略掉了，所以txt文件一直都没有被读取。将代码改成如下形式，release版本下也能正常读取了。 1234567891011121314151617void Config::gen_data_list()&#123; QFile f(select_dataset_path); bool isOpen &#x3D; f.open(QIODevice::ReadOnly | QIODevice::Text); assert(isOpen); while (!f.atEnd()) &#123; QString l &#x3D; f.readLine(); qDebug() &lt;&lt; l &lt;&lt; endl; if (l.compare(&#39;\\n&#39;) &#x3D;&#x3D; 0) continue; else data_list.append(l.simplified()); &#125; data_num &#x3D; data_list.length(); trial_num &#x3D; data_num * blk_num;&#125; 这样isOpen的值取决于文件是否打开。所以f.open(QIODevice::ReadOnly | QIODevice::Text)这部分的内容一定会被执行的。在VS中，也可以在属性中打开编译调试代码开关，这样就会编译assert函数了。 原因探究查阅相关资料后，我发现assert语句在windows下，利用VC的编译器时，会被忽略掉而不执行。这取决于release和debug时的编译器优化方式。在linux条件下，使用gcc编译时则不会忽略assert语句。其他深入的编译原理相关的原因就不再接着探究了。总的来说，不管使用何种编译器，代码规范化是很重要的。assert语句不应该被用来检测文件读取，以及输入是否合法等问题。也不能把赋值操作等语句放在其中。总之，检查代码的时候，把assert语句去掉，如果代码的正常逻辑没有问题。那么代码就是正常的。assert应该是用来检测参数的合法性以及参数值的大小等涉及代码完备性和安全性的问题。 问题2问题描述在搭建EMG信号处理系统时，需要通过state参数来判断是否暂停函数中的for循环。在debug条件下，暂停和恢复都能正常运行。暂停条件下停止实验也能正常运行，但是在release条件下，暂停后就没法恢复正常运行了，而且暂停后要么会在下一个trial时停止，要么在后面恢复时，没有响应。代码块如下： 123456789101112131415for (int i &#x3D; 1; i &lt;&#x3D; exp_c.blk_num; i++)&#123; for (int j &#x3D; 1; j &lt;&#x3D; exp_c.data_num; j++)&#123; if (th_record_state &#x3D;&#x3D; 2)&#123; while (true)&#123; if (main_th_stop)&#123; stop_experiment(); break; &#125; if (th_record_state &#x3D;&#x3D; 1) break; &#125; &#125; for (int k &#x3D; 1; k &lt;&#x3D; state_num; k++)&#123; Process(); &#125; 需要实现的功能是，在th_record_state为1（record state）时，实验正常进行；在th_record_state为2（pause state）时，实验暂停；在th_record_state为0时，main_th_stop为true，实验结束。 解决办法由于release版本下，VS的优化器认为while循环中的内容对外部代码没有影响，且while循环会占用很多计算量。所以在release版本下，while循环中的代码会被忽略掉。所以在while循环中加入延时，减少因为暂停导致的循环次数。这样优化器会重新加入while循环的内容。（这部分涉及到VS中的编译优化的问题，没有深究原因）。修改后的代码如下： 12345678910111213141516for (int i &#x3D; 1; i &lt;&#x3D; exp_c.blk_num; i++)&#123; for (int j &#x3D; 1; j &lt;&#x3D; exp_c.data_num; j++)&#123; if (th_record_state &#x3D;&#x3D; 2)&#123; while (true)&#123; Sleep(100); if (main_th_stop)&#123; stop_experiment(); break; &#125; if (th_record_state &#x3D;&#x3D; 1) break; &#125; &#125; for (int k &#x3D; 1; k &lt;&#x3D; state_num; k++)&#123; Process(); &#125; 尽管这样会损失一些反应时间，但是实验系统对反应时间的要求也不高，而且Sleep函数的时间可以根据实验所需的反应时间修改。这种解决方案也可以接受。 原因探究主要的原因还是debug和release版本下，VS的优化方法不一样。优化的参数可以在项目属性中调整。但是release版本的代码对稳定性的要求会更高，类似于数组越界，指针赋值等问题，在debug时可能没问题。但是release时就会出现问题。此时一定要检查代码的规范性问题，以及代码中的一些合理性问题，例如本文中出现的assert语句中加入文件读取语句，while循环不加延时导致大量无效循环等问题。","categories":[{"name":"coding","slug":"coding","permalink":"http://frankmartinem.github.io/categories/coding/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://frankmartinem.github.io/tags/C/"}]},{"title":"powershell commands-tree","slug":"powershell-commands-tree","date":"2021-06-10T15:16:00.000Z","updated":"2021-06-10T15:49:48.779Z","comments":true,"path":"2021/06/10/powershell-commands-tree/","link":"","permalink":"http://frankmartinem.github.io/2021/06/10/powershell-commands-tree/","excerpt":"","text":"powershell命令：tree显示文件夹中的文件结构，并生成txt文件或md文件。用法如下： 1234tree [drive:][path] [&#x2F;F] [&#x2F;A] &gt;[PRN][&#x2F;F]: 显示目录下的文件名[&#x2F;A]: 使用ASCII码字符[PRN]: 存储生成的文件结构的txt或md文件","categories":[{"name":"powershell","slug":"powershell","permalink":"http://frankmartinem.github.io/categories/powershell/"}],"tags":[{"name":"software","slug":"software","permalink":"http://frankmartinem.github.io/tags/software/"}]},{"title":"Linux Environment Variable","slug":"Linux-Environment-Variable","date":"2021-06-01T03:02:23.000Z","updated":"2021-06-01T03:07:44.468Z","comments":true,"path":"2021/06/01/Linux-Environment-Variable/","link":"","permalink":"http://frankmartinem.github.io/2021/06/01/Linux-Environment-Variable/","excerpt":"","text":"Linux 环境变量设置Linux环境变量在/.bashrc文件中设置。也可能在/.zshrc，也可能在/etc/profile中。设置的方法是 1export PATH&#x3D;$PATH:&#x2F;yourpath&#x2F; 一般是放在所有环境变量最后的。也可以放在最前面。我安装texlive的时候，放在最前面的写法没有反应。放在最后面能正常运行pdflatex等命令。","categories":[{"name":"usage","slug":"usage","permalink":"http://frankmartinem.github.io/categories/usage/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankmartinem.github.io/tags/Linux/"}]},{"title":"visual studio code cpp configure","slug":"visual-studio-code-cpp-configure","date":"2021-05-31T12:52:53.000Z","updated":"2021-05-31T15:06:32.359Z","comments":true,"path":"2021/05/31/visual-studio-code-cpp-configure/","link":"","permalink":"http://frankmartinem.github.io/2021/05/31/visual-studio-code-cpp-configure/","excerpt":"","text":"Visual Studio Code 配置c++编译环境原因平时写c++大部分时间是在windows环境下，然后用Visual Studio调试和编译代码。但是由于笔记本的系统是macOS，有时候需要远程调试代码。远程调试用Teamviewer或者Microsoft Remote Desktop的话，还是不太方便。再加上也想学习一下g++编译器。相较于msvc，g++跨平台的特性更加实用。 配置 安装g++编译器（windows：MinGW，linux：sudo安装，macOS: brew安装）。必装的项目有g++，gcc，gdb， vscode安装C++扩展（C/C++） 配置lanuch.json和tasks.json文件。以下是标准的launch.json文件和tasks.json文件123456789101112131415161718192021222324lanuch.json&quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;生成和调试活动文件&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;$&#123;workspaceFolder&#125;&#x2F;$&#123;fileBasenameNoExtension&#125;.exe&quot;, &quot;args&quot;: [], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: true, &quot;MIMode&quot;: &quot;gdb&quot;, &#x2F;&#x2F;调试模式 &quot;miDebuggerPath&quot;: &quot;D:\\\\MinGW\\\\bin\\\\gdb.exe&quot;, &quot;preLaunchTask&quot;: &quot;g++&quot;, &quot;setupCommands&quot;: [ &#123; &quot;description&quot;: &quot;为 gdb 启用整齐打印&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: false &#125; ] &#125; ] 12345678910111213141516171819202122232425tasks.json&#123; &quot;version&quot;: &quot;2.0.0&quot;, &quot;tasks&quot;: [ &#123; &quot;type&quot;: &quot;shell&quot;, &quot;label&quot;: &quot;g++&quot;, &quot;command&quot;: &quot;D:\\\\MinGW\\\\bin\\\\g++.exe&quot;, &quot;args&quot;: [ &quot;-g&quot;, &quot;$&#123;file&#125;&quot;, &quot;-o&quot;, &quot;$&#123;workspaceFolder&#125;\\\\$&#123;fileBasenameNoExtension&#125;.exe&quot; ], &quot;options&quot;: &#123; &quot;cwd&quot;: &quot;D:\\\\MinGW\\\\bin&quot; &#125;, &quot;problemMatcher&quot;: [ &quot;$gcc&quot; ], &quot;group&quot;: &quot;build&quot;, &quot;detail&quot;: &quot;调试器生成的任务。&quot; &#x2F;&#x2F;注意和lanuch.json中的文件匹配 &#125;, ],&#125; 然后就是写代码编译生成可执行文件了。这部分和g++编译器的内容相关。后面继续学习g++编译器。","categories":[{"name":"configuration","slug":"configuration","permalink":"http://frankmartinem.github.io/categories/configuration/"}],"tags":[{"name":"software","slug":"software","permalink":"http://frankmartinem.github.io/tags/software/"}]},{"title":"Optimal Linear Estimation","slug":"Optimal-Linear-Estimation","date":"2021-05-28T08:50:27.000Z","updated":"2021-07-24T17:35:53.924Z","comments":true,"path":"2021/05/28/Optimal-Linear-Estimation/","link":"","permalink":"http://frankmartinem.github.io/2021/05/28/Optimal-Linear-Estimation/","excerpt":"","text":"Optimal Linear Estimation 背景 最优线性估计算法是神经解码中一种比较常用的算法。在算法刚提出来的时候，其解码精度和解码速度都属于较高的水准，因此在脑机接口实验中应用广泛。随着神经网络的兴起以及传统机器学习算法的更新，BCI领域用来解码的算法也越来越多，例如KF，UKF，RNN，CNN等。OLE尽管计算精度不如目前的算法，但是计算量小，反馈迅速。因此目前在线的BCI实验OLE的应用仍然较多。 算法推导 OLE算法是PVA算法的改进，PVA算法在之前的Blog中有提到，是BCI中应用最早的算法。但是PVA有自己的缺陷，即很依赖数据的质量。这里的质量指的是用于解码的神经元集群的偏好方向分布。如果偏好方向的分布不均匀，朝某个方向的神经元占大多数，那么解码得到的方向就会偏向于这个方向，导致朝其他方向的运动很困难。为了解决这个问题，Chase等人提出了对于PVA算法的改进方法，即OLE算法[1]。 OLE算法的核心思想就是利用线性插值的方法，把神经元的偏好方向调整到尽量在各个方向都是均匀分布的。我们假设有2个神经元，偏好方向如图1中红色和蓝色的虚线所示。当朝各个方向运动时，神经元的发放率变化程度会不一样。当朝着神经元偏好方向运动时，神经元会更活跃，朝反方向运动时，会更加被抑制。但是当朝着垂直于偏好方向的方向运动时，神经元的发放率不会有明显变化，此时，解码误差会很大，或者说，很难解码到朝这个方向的运动。 为了便于理解这个问题，我们可以用一个更极端的假设，即所有神经元的偏好方向都朝向x轴正方向，那么此时对于y轴的运动，是无法通过神经元解码得到的。PVA的计算公式里，y轴运动的参数b1b_1b​1​​是0。这里有一个需要理解的概念，即神经元集群的解码，不是取决于神经元的发放率，而是发放率的变化。朝哪个方向运动能有神经元有强烈的发放率变化，那么朝这个方向的运动解码就准确。 为了解决上述问题，Chase等人提出了OLE算法，具体的计算方法如下： 假设神经元的发放率为r(t)r(t)r(t)。神经元的偏好方向矩阵为BBB，当前的运动方向为d(t)d(t)d(t)。那么有： r(t)=B∗d(t)+ϵ(t)r(t) = B * d(t) + \\epsilon(t) r(t)=B∗d(t)+ϵ(t) 其中ϵ(t)\\epsilon(t)ϵ(t)表示ttt时刻的噪声。假设神经元的个数为NNN，那么r(t)∈RNx1r(t) \\in R^{Nx1}r(t)∈R​Nx1​​。假设解码的维度为ddd，那么B∈RNx(d+1)B \\in R^{Nx(d+1)}B∈R​Nx(d+1)​​。这里加1表示常数项。 那么，预测的运动方向为： dpred(t)=(B′B)−1B′r(t)d_{pred}(t) = (B&#x27;B)^{-1}B&#x27;r(t) d​pred​​(t)=(B​′​​B)​−1​​B​′​​r(t) 以上就是OLE算法的计算内容。和PVA算法比较，似乎没什么太大的差别。但是思想是不同的。 首先，PVA的计算，前提假设就包括了神经元的分布是均匀的。体现在这里，即B′B=IB&#x27;B=IB​′​​B=I，其中III表示单位矩阵。那么上述公式可以写为：dpred(t)=B′r(t)d_{pred}(t) = B&#x27;r(t)d​pred​​(t)=B​′​​r(t)。即PVA的计算方法，单独计算每个神经元的发放率，然后计算在当前偏好方向的投影，然后求和之后得到预测的运动方向。 对于OLE的计算，更加像是先计算了神经元分布的均匀度。然后根据不同方向的运动权重重新分布当前的偏好方向。使得神经元分布更加均匀。即B′BB&#x27;BB​′​​B这个矩阵的计算值，也就是运动维度的协方差。这里举个例子，假设所有神经元的偏好方向都是x轴正方向，那么B′B=[[1,1],[0,0]]B&#x27;B=[[1, 1], [0, 0]]B​′​​B=[[1,1],[0,0]]。 这个时候x轴和y轴的运动都会存在。即我们把神经元的偏好方向从x轴正方向旋转了45度。Chase的文章中的图可以很好的解释这个原理： 需要注意的是，BBB矩阵的计算方法和PVA算法是一致的。 以上就是OLE算法的计算过程了，OLE的计算方法和PVA很像，但是解决了神经元分布不均匀的问题。这个问题在BCI中很常见，所以OLE相较于PVA，效果一般都是会更好的。还有一种改进版的OLE算法-‘full OLE’。之前介绍的这种是’minimal OLE’。 ‘full OLE’ 相较于’minimal OLE’的区别在于其假设了神经信号中存在了同源或相似的噪声。那么在计算过程中，这种噪声会体现在解码的结果上，导致运动方向产生误差。其改进方法也很简单，只是在预测公式中，加入了所有通道神经元的协方差矩阵，如下： dpred(t)=(B′ΣB)−1B′r(t)d_{pred}(t) = (B&#x27;\\Sigma B)^{-1}B&#x27;r(t) d​pred​​(t)=(B​′​​ΣB)​−1​​B​′​​r(t) 这里的Σ\\SigmaΣ就是协方差矩阵，如果神经信号之间没有相关性，即没有同源噪声的话，Σ=I\\Sigma = IΣ=I。也就是’minial OLE’的计算方法了。 代码 后续会附上代码链接 [1] S. M. Chase, A. B. Schwartz, and R. E. Kass, “Bias, optimal linear estimation, and the differences between open-loop simulation and closed-loop performance of spiking-based brain–computer interface algorithms,” Neural networks, vol. 22, no. 9, pp. 1203-1213, 2009.","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankmartinem.github.io/categories/Algorithm/"}],"tags":[{"name":"Optimal Linear Estimation","slug":"Optimal-Linear-Estimation","permalink":"http://frankmartinem.github.io/tags/Optimal-Linear-Estimation/"}]},{"title":"ZJU RVPN initialize failed","slug":"ZJU RVPN Initialize Failed","date":"2021-05-03T10:29:42.000Z","updated":"2021-05-03T10:37:42.463Z","comments":true,"path":"2021/05/03/ZJU RVPN Initialize Failed/","link":"","permalink":"http://frankmartinem.github.io/2021/05/03/ZJU%20RVPN%20Initialize%20Failed/","excerpt":"","text":"ZJU RVPN 初始化失败的解决办法问题描述在 macOS系统，遇见easyconnect一直提示初始化失败的问题。重新安装后仍旧不能解决问题。后发现是macOS系统开机时禁止启动了两个easyconnect的进程。分别为： com.sangfor.EasyMonitor.plist com.sangfor.ECAgentProxy.plist禁止后easyconnect无法启动代理，连接校内网络 解决办法用软件Tencent Lemon设置开机启动项，在“未知应用“选项中找到对应的进程，打开开机启动项。然后重启电脑，即可解决问题","categories":[{"name":"software","slug":"software","permalink":"http://frankmartinem.github.io/categories/software/"}],"tags":[{"name":"software usage","slug":"software-usage","permalink":"http://frankmartinem.github.io/tags/software-usage/"}]},{"title":"Berkeley-CS-61A","slug":"Berkeley-CS-61A","date":"2020-05-16T09:01:22.000Z","updated":"2021-04-13T00:59:38.113Z","comments":true,"path":"2020/05/16/Berkeley-CS-61A/","link":"","permalink":"http://frankmartinem.github.io/2020/05/16/Berkeley-CS-61A/","excerpt":"","text":"Berkeley CS 61CLecture-1 不是所有的问题都能用计算机解决，也不是所有的问题，用计算机解决更加方便 学会抽象的看待事物，不需要了解其中的详细构造","categories":[{"name":"Lecture","slug":"Lecture","permalink":"http://frankmartinem.github.io/categories/Lecture/"}],"tags":[{"name":"Lecture","slug":"Lecture","permalink":"http://frankmartinem.github.io/tags/Lecture/"},{"name":"Computer Science","slug":"Computer-Science","permalink":"http://frankmartinem.github.io/tags/Computer-Science/"}]},{"title":"Linear Square Method","slug":"Linear-Square-Method","date":"2020-04-16T17:09:41.000Z","updated":"2021-04-13T01:02:10.124Z","comments":true,"path":"2020/04/17/Linear-Square-Method/","link":"","permalink":"http://frankmartinem.github.io/2020/04/17/Linear-Square-Method/","excerpt":"","text":"最小二乘法这几天看书的时候突然注意到了这个经典的优化方法，于是重新推导了一遍，为以后应用做参考。 背景最小二乘法应该是我接触的最早的优化方法，也是求解线性回归的一种方法。线性回归的主要作用是用拟合的方式，求解两组变量之间的线性关系（当然也可以不是线性的，那就是另外的回归方法了）。也就是把一个系统的输出写成输入的线性组合的形式。而这组线性关系的参数求解方法，就是最小二乘法。 我们从最简单的线性回归开始，即输入和输出都是1维的。此时，最小二乘法也是最简单的。 假设有输入信号$x = {x_0, x_1, …, x_t}$，同时输出信号为$y = {y_0, y_1, …, y_t}$，我们假设输入信号$x$和输出信号$y$之间的关系可以写成如下形式： $$y_{pre} = ax+b \\tag{1}$$ 我们需要求解最优的$a$和$b$，这里最优的含义就是，预测的最准确，也就是预测值和真实值的误差最小，即： $$arg, min_{a, b}{\\sum_{i=0}^{t}{(y_i-ax_i-b)^2}} \\tag{2}$$ 我们假设误差函数为： $$err = \\sum_{i=0}^{t}{(y_i-ax_i-b)^2} \\tag{3}$$ $err$对$a$和$b$分别求偏导： $$\\frac{\\partial{err}}{\\partial{a}} = \\sum_{i=0}^{t}{2(ax_i+b-y_i)*x_i} \\tag{4}$$ $$\\frac{\\partial{err}}{\\partial{b}} = \\sum_{i=0}^{t}{2(ax_i+b-y_i)} \\tag{5}$$ 根据极值定理，有$$\\frac{\\partial{err}}{\\partial{a}}=0$$，且$$\\frac{\\partial{err}}{\\partial{b}}=0$$，所以有： $$\\sum_{i=0}^{t}{2(ax_i+b-y_i)} = 0 \\tag{6}$$ $$\\sum_{i=0}^{t}(y_i - ax_i) = \\sum_{i=0}^{t}{b} \\tag{7}$$ $$\\sum_{i=0}^{t}{y_i} - a * \\sum_{i=0}^{t}{x_i} = (t+1)*b \\tag{8}$$ $$b = \\bar{y} - a\\bar{x} \\tag{9}$$ 其中，$\\bar{y}$表示$y$的均值，$\\bar{x}$表示$x$的均值。将Eq(9)代入Eq(4)，有： $$\\sum_{i=0}^{t}{2(ax_i+b-y_i)*x_i} = 0 \\tag{10}$$ $$\\sum_{i=0}^{t}{ax_i^2} + \\sum_{i=0}^{t}bx_i = \\sum_{i=0}^{t}{y_ix_i} \\tag{11}$$ $$a\\sum_{i=0}^{t}x_i^2 + \\bar{x}(\\bar{y}-a\\bar{x}) = \\sum_{i=0}^{t}{x_iy_i} \\tag{12}$$ $$a(\\sum_{i=0}^{t}{x_i^2 - \\bar{x}^2}) = \\sum_{i=0}^{t}{x_iy_i}-\\bar{x}\\bar{y} \\tag{13}$$ $$a = \\frac{\\sum_{i=0}^{t}{x_iy_i}-\\bar{x}\\bar{y}}{\\sum_{i=0}^{t}{x_i^2 - \\bar{x}^2}} \\tag{14}$$ 所以Eq(14)和Eq(9)就是最简单的最小二乘法的计算方法。 然后我们进一步考虑，如果输入和输出是多维数据，要如何计算。 假设输入信号为$X \\in R^{mt}$， 输出信号为$Y \\in R^{nt}$，那么有： $$Y = W_0X+B = WX_1 \\tag{15}$$ 其中$W_0 \\in R^{nm}$是回归矩阵的系数，$B \\in R^{1t}$表示常数项，这里可以直接写到$W$矩阵中。$W \\in R^{(m+1)*t}$，$X_1 \\in R^{(m+1)*t}$$$X_1 = \\begin{bmatrix}x_{11} &amp;x_{12} &amp; … &amp;x_{1t}\\x_{11} &amp;x_{12} &amp; … &amp;x_{1t}\\{\\vdots} &amp;{\\vdots} &amp;… &amp;{\\vdots}\\x_{m1} &amp;x_{m2} &amp;… &amp;x_{mt}\\1 &amp;1 &amp;… &amp;1\\\\end{bmatrix} \\tag{16}$$ 所以有： $$\\arg min_{W}({Y-WX_1}) \\tag{17}$$ 假设误差函数为$E$，则有： $$E = (Y-WX_1)(Y-WX_1)^T = YY^T - WX_1Y^T-YX_1^TW^T+WX_1X_1^TW^T \\tag{18}$$ 计算$E$对$W$的偏导，则该偏导等于0： $$\\frac{\\partial{E}}{\\partial{W}} = -X_1Y^T-X_1^TY + 2WXX^T = 0 \\tag{19}$$ 所以有： $$W = (X_1X_1^T)^{-1}X_1Y^T \\tag{20}$$ 至此矩阵形式的最小二乘法（多元线性回归的参数解法）推导完成。注意这里的$X_1$和$Y$中的数据排列方式为：每一行是一个维度的数据，每一列表示一个时间点。如果不是这么记录的话，那么公式需要加上转置。 后续会附上代码链接","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankmartinem.github.io/categories/Algorithm/"}],"tags":[{"name":"Linear Regression","slug":"Linear-Regression","permalink":"http://frankmartinem.github.io/tags/Linear-Regression/"}]},{"title":"Wiener-Filter","slug":"Wiener-Filter","date":"2020-04-13T09:32:08.000Z","updated":"2021-07-24T17:38:43.571Z","comments":true,"path":"2020/04/13/Wiener-Filter/","link":"","permalink":"http://frankmartinem.github.io/2020/04/13/Wiener-Filter/","excerpt":"","text":"Wiener Filter 因为最近看文章接触了维纳滤波，所以这里写一下Weiner Filter的一些简单理解和推导。 基本定义 维纳滤波是一种在含噪声的时序信号把信号提取出来的滤波器，其基本框图如下： 简单的维纳滤波其实就是通过一个FIR滤波器，去除噪声的过程。在这里，hhh的作用也可以理解为： 通过训练集的数据对信号和噪声的建模，然后通过前几个点的信息，预测当前时刻的噪声信号所占的比例，然后去除掉，剩下的就是预测的时序信号了。维纳滤波作为一种使用很广泛的滤波器，其变化的形式也有很多种，可以是单输入输出的，也可以是多输入输出的。hhh所表示的变换也可以写成非线性；hhh可以是有限长的FIR滤波器，也可以是无限长的IIR滤波器。要取决于当前你所解决的问题。但是维纳滤波的基本思想还是一致的。通过滤波（矩阵或者其他模型的形式）来从信号和噪声的混合中提取信号。所以维纳滤波的核心，就是计算这个滤波器（矩阵hhh或者模型的参数）。也就是解Wiener-Hopf方程。 本文用比较简单的单输入输出，且只考虑有限长滤波（即认为当前时刻的信号只和前有限个时间点的信号相关）。 公式推导 首先，对于图1中的滤波器： y(n) = x(n) * h(n) = (s(n)+v(n))*h(n) \\tag{1} 其中∗*∗表示卷积，x(n)x(n)x(n)表示输入信号， y(n)y(n)y(n)表示输出信号， s(n)s(n)s(n)表示输入信号中，有用的信号部分；v(n)v(n)v(n)表示输入信号中的噪声部分。 维纳滤波的目标是，保证输出y(n)y(n)y(n)和真实信号s(n)s(n)s(n)的差别最小，由于y(n)y(n)y(n)和s(n)s(n)s(n)是时序信号，所以要保证两者的均方误差最小，所以有： E\\{e^2(n)\\} = E\\{(y(n)-s(n))^2\\} = E\\{(x(n)*h(n)-s(n))^2\\} \\tag{2} 即求使得Eq(2)最小的hhh。所以E{e2}E\\{e^2\\}E{e​2​​}对hhh求偏导。有： \\frac{\\partial{E\\{e^2(n)\\}}}{\\partial{h}} = 2E\\{e(n) * \\frac{\\partial{e(n)}}{\\partial{h}}\\} = 0 \\tag{3} \\frac{\\partial{E\\{e^2(n)\\}}}{\\partial{h}} = 2E\\{[\\sum_{m=0}^{N-1}{h(m)x(n-m) - s(n)}]x(n-j)\\}, j = 0, 1, ... , N-1 \\tag{4} \\frac{\\partial{E\\{e^2(n)\\}}}{\\partial{h}} = 2\\sum_{m=1}^{N-1}{h(m)}E\\{x(n-j)x(n-m)\\} - 2E\\{s(n)x(n-j)\\} = 0, j = 0, 1, ..., N-1 \\tag{5} 我们设xxx和sss的相关系数为RxsR_{xs}R​xs​​，则有： R_{xs}(j)=\\sum_{m=0}^{N-1}{h(m)R_{xx}(j-m)}, j=0,1,...,N-1 \\tag{6} 其中，Rxx(j−m)R_{xx}(j-m)R​xx​​(j−m)表示x(n−j)x(n-j)x(n−j)和x(n−m)x(n-m)x(n−m)的相关系数，这里mmm是固定的，jjj是变化的。且m&gt;=0m&gt;=0m&gt;=0，Rxs(j)R_{xs}(j)R​xs​​(j)表示x(n−j)x(n-j)x(n−j)和s(n)s(n)s(n)的相关系数。上述公式中，nnn表示的是时序信号中的时间点。 然后，根据Eq(6)，可以得到NNN个线性方程： \\begin{cases} R_{xs}(0)=h(0)R_{xx}(0)+h(1)R_{xx}(1)+...+h(N-1)R_{xx}(N-1)\\\\ R_{xs}(1)=h(1)R_{xx}(1)+h(0)R_{xx}(0)+...+h(N-1)R_{xx}(N-2)\\\\ ...\\\\ R_{xs}(N-1)=h(N-1)R_{xx}(N-1)+h(N-2)R_{xx}(N-2)+...+h(0)R_{xx}(0)\\\\ \\end{cases} \\tag{7} 写成矩阵形式，有： \\displaystyle \\boldsymbol{R_{xx}H}=\\boldsymbol{R_{xs}} \\tag{8} 其中， \\displaystyle \\boldsymbol{H} = [h(0), h(1),...,h(N-1)]^T是需要求的滤波器参数 \\displaystyle \\boldsymbol{R_{xs}} = [R_{xs}(0),R_{xs}(1), ..., R_{xs}(N-1)]^T$$是$x$和$s$的相关系数 \\displaystyle \\boldsymbol{R_{xx}} = \\begin{bmatrix} R_{xx}(0)&amp;R_{xx}(1)&amp;…&amp;R_{xx}(N-1)\\ R_{xx}(1)&amp;R_{xx}(0)&amp;…&amp;R_{xx}(N-2)\\ {\\vdots}&amp;{\\vdots}&amp;…&amp;{\\vdots}&amp;\\ R_{xx}(N-1)&amp;R_{xx}(N-2)&amp;…&amp;R_{xx}(0)\\ \\end{bmatrix} \\tag{9} 所以根据Eq(8)可以求得： $$\\displaystyle \\boldsymbol{H} = \\boldsymbol{R_{xx}^{-1}R_{xs}} \\tag{10} 此时，信号的均方误差最小，根据Eq(2)，可得： E\\{e^2(n)\\} = E\\{(s(n)-\\sum_{m=0}^{N-1}h(m)x(n-m))^2\\} \\tag{11} E{e2(n)}=E{s2(n)−2s(n)∑m=0N−1h(m)x(n−m)+∑m=0N−1∑r=0N−1h(m)x(n−m)h(r)x(n−r)}E\\{e^2(n)\\} = E\\{s^2(n) - 2s(n)\\sum_{m=0}^{N-1}h(m)x(n-m)+\\sum_{m=0}^{N-1}\\sum_{r=0}^{N-1}{h(m)x(n-m)h(r)x(n-r)}\\} E{e​2​​(n)}=E{s​2​​(n)−2s(n)​m=0​∑​N−1​​h(m)x(n−m)+​m=0​∑​N−1​​​r=0​∑​N−1​​h(m)x(n−m)h(r)x(n−r)} E{e2(n)}=Rss(0)−2∑m=0N−1h(m)Rxs(m)+∑m=0N−1h(m)∑r=0N−1h(r)Rxx(n−r)E\\{e^2(n)\\}=R_{ss}(0)-2\\sum_{m=0}^{N-1}{h(m)R_{xs}(m)+\\sum_{m=0}^{N-1}{h(m)}\\sum_{r=0}^{N-1}{h(r)R_{xx}(n-r)}} E{e​2​​(n)}=R​ss​​(0)−2​m=0​∑​N−1​​h(m)R​xs​​(m)+​m=0​∑​N−1​​h(m)​r=0​∑​N−1​​h(r)R​xx​​(n−r) 根据Eq(5)，可得： E\\{e^2(n)\\} = R_{ss}(0) - \\sum_{m=0}^{N-1}{h(m)R_{xs}(n-m)} \\tag{12} 假设信号s(n)s(n)s(n)和噪声v(n)v(n)v(n)互相独立，那么有： Rsv=Rvs=0R_{sv}= R_{vs} = 0 R​sv​​=R​vs​​=0 Rxs=Rss+Rvs=RssR_{xs} = R_{ss} + R_{vs} = R_{ss} R​xs​​=R​ss​​+R​vs​​=R​ss​​ Rxx=Rss+Rsv+Rvs+Rvv=Rss+RvvR_{xx} = R_{ss}+R_{sv}+R_{vs}+R_{vv} = R_{ss}+R_{vv} R​xx​​=R​ss​​+R​sv​​+R​vs​​+R​vv​​=R​ss​​+R​vv​​ 则，根据Eq(12)，有： E\\{e^2(n)\\} = R_{ss}(0) - \\sum_{m=0}^{N-1}{h(m)R_{ss}(m)} \\tag{14} 至此，最简单的维纳滤波的基本公式推导完成，如果涉及到多输入多输出的维纳滤波，会更加复杂，这里不做推导。后续会附上代码链接","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankmartinem.github.io/categories/Algorithm/"}],"tags":[{"name":"Wiener Filter","slug":"Wiener-Filter","permalink":"http://frankmartinem.github.io/tags/Wiener-Filter/"}]},{"title":"python_note","slug":"python-note","date":"2020-03-20T07:00:48.000Z","updated":"2020-03-24T02:46:16.000Z","comments":true,"path":"2020/03/20/python-note/","link":"","permalink":"http://frankmartinem.github.io/2020/03/20/python-note/","excerpt":"","text":"PythonBasic Knowledge python中新建一个变量并赋值，在计算机中是怎么处理的 1x = 1 例如上述代码，计算机首先是开辟一块内存 ，如果是C++，Java等静态语言，那么会根据变量类型指定内存大小。如果是python等动态语言，则智能开辟内存大小，在内存中写入这一数据；但是计算机到目前为止，还不能将x和该值绑定，所以计算机中还会开辟一块内存，名字为x，然后将这一内存指向刚才开辟的内存地址。 python中没有指针的概念，例如下面的代码 123x = 1y = xx = 2 在python中，这一过程是这样的： 开辟内存，存入1，开辟内存，存入x，并将x指向刚才开辟的存入1的内存 开辟内存，存入y，并将y也指向x指向的那块内存 开辟内存，存入2，并将x指向新开的地址 如果是在C++, Java等有指针概念的语言中，第二步应该是这样的，开辟新的内存，将x指向的地址的值复制一份存入新地址，开辟内存，存入y，并指向新地址 python循环 python中的循环有如下2种方式： for循环 python中的for循环的写法是for x in y, 也就是遍历数组y中的所有变量，提取出来的数是x，在数值计算中运用最多的是for x in range(1, 100), 这种方法是定义了一个1到100的数组，通过range函数，在内存中存储这样的数组，再遍历 while循环 这种方法和其他语言类似，不再赘述 python函数 在python中，函数名是指向函数对象的一个引用，所以在python中，可以把函数名赋值给一个变量，这有点类似于matlab中的@func句柄的意思。 也就是说在python中，函数名对应的内存中，只存储了函数的名字以及队医你个Object的地址，所以可以幅值给另外一个变量。真正的函数对象是存储在另外的内存中的 函数的默认参数 123def add_end(L=[]): L.append(&#x27;END&#x27;) return L 这样一个函数，用默认参数调用一次，结果是[‘END’]，调用两次，结果是[‘END’, ‘END’]，这是因为，函数在运行时，默认参数也是存放在一块内存中的，每次调用时，由于内存 能够写入，所以每次运行完函数后，默认参数对应的内存都会刷新。所以python中，默认参数最好写成不变的量。 python迭代器 python中可以通过列表生成式的方式得到（类似于Matlab中的矩阵运算）。但是列表很大的时候，而且列表的数据很有规律（例如range(10000000)），但是又只需要用到列表中的少量数据，那么可以用迭代器(generator)的方法，描述列表的数字规律，当需要某个数时，根据规律计算得到，就不需要占用很大的内存了。 函数闭包(closure) 一个函数将另外一个函数作为返回值，并且在返回的函数中存储了内部的局部变量，这种方法称为闭包。闭包能保存原有的局部变量，在调用函数时引用，应用范围较广。但是闭包的应用中，要注意不能引用循环变量。因为闭包返回的函数是等所有函数都返回了才执行，而当所有函数都返回时，循环变量已经变为最终值。 偏函数 偏函数存在于functiontools中，主要的作用就是固定函数的某些参数，其返回值是一个函数。 线程锁 线程与进程的一个不同之处在于，进程之间的变量是独立的，如果两个进程同时读写某一内存，那么用Queue或者Pipe来保证读写的顺序。同时，进程只是把数据读取到自己的运算域内，相当于把数据拷贝一份。但是线程不一样，线程之间的变量是共享的，所以会导致线程对变量的赋值混乱。所以我们需要用线程锁。获得线程锁的线程，会保证在运行期间，变量只能由该线程修改。从而保证变量不会混乱。但是线程锁用完了一定要释放，最好用try…finally保证一定会释放。不然其他线程永远不能修改。但是线程锁也有缺陷，在一些并发线程中并不适用。 GIL锁 在Python的解释器中，有GIL(Global Interpreter Lock)。Python的线程在执行前，一定要获得GIL锁，然后一定时间后，解释器释放GIL锁，然后其他线程再获得GIL锁。这样会保证，同一个进程的多个线程，最多只能用到CPU的一个核。而不能跑满CPU。如果要充分利用CPU，那么可以用多进程的方式，或者用其他的语言实现，例如C，Java等。 正则表达式 正则表达式的基本表达方式 标签 含义 \\d 匹配1个数字 \\w 匹配1个字母或1个数字 . 可以匹配任何符号 ×(star) 表示任意长度的字符 + 表示至少1个字符 ? 表示0个或1个字符 {n} 表示n个字符 {n,m} 表示n-m个字符 \\s 匹配一个空格 [] 用来表示范围 | 表示或 ^ 匹配一行的开头 $ 匹配一行的结尾 () 表示分组 正则表达式一般采用贪婪匹配，也就是说，如果前面的表达式符合要求，会一直匹配到不符合要求的那个数字，有可能导致后面的表达式匹配到空字符串。如果不想用贪婪匹配，那么需要在每一段表达式后面加上?。 难理解的点 元类，metaclass 装饰器，decorator","categories":[{"name":"coding","slug":"coding","permalink":"http://frankmartinem.github.io/categories/coding/"}],"tags":[{"name":"python","slug":"python","permalink":"http://frankmartinem.github.io/tags/python/"}]},{"title":"Linear Algebra","slug":"Linear-Algebra","date":"2020-02-23T05:32:52.000Z","updated":"2021-04-13T01:01:43.998Z","comments":true,"path":"2020/02/23/Linear-Algebra/","link":"","permalink":"http://frankmartinem.github.io/2020/02/23/Linear-Algebra/","excerpt":"","text":"线性代数本文主要记录学习过程中遇到的线性代数的基本概念以及公式 Basic Concept行列式的计算矩阵的行列式等于矩阵中任意一行的值","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"http://frankmartinem.github.io/categories/Mathematics/"}],"tags":[{"name":"Linear Algebra","slug":"Linear-Algebra","permalink":"http://frankmartinem.github.io/tags/Linear-Algebra/"}]},{"title":"Gradient Descent Method","slug":"Gradient-Descent-Method","date":"2020-02-06T07:25:52.000Z","updated":"2021-04-13T01:01:07.245Z","comments":true,"path":"2020/02/06/Gradient-Descent-Method/","link":"","permalink":"http://frankmartinem.github.io/2020/02/06/Gradient-Descent-Method/","excerpt":"","text":"梯度下降法这篇Blog的主要内容是关于梯度下降法的一些理解，以及相关的公式推导。梯度下降法很早之前就接触过，但是因为长时间不用，所以理解上也有了一些欠缺，今天看了一些参考文献，写一下自己的一些理解。便于以后帮助自己回忆。 Artificial Neural Network关于人工神经网络，这是目前使用最广泛的一类算法了。神经网络和其他的算法相比较，计算更加直接。不需要去推导公式，去计算两者的关系，直接通过网络的方式连接，然后用大量的数据训练，没有关系的连接权重逐渐变弱，有关系的权重逐渐变强。如果把输入和输出的函数关系写出来，会发现是一个很复杂的非线性公式。也正是因为这一点，神经网络的拟合程度比普通的线性，非线性算法都要好。 Gradient Descent对于用梯度下降法训练神经网络，我之前一直没有弄明白的点是为什么梯度的方向就是函数增加最快的方向， 我理解梯度方向是变化最快的方向，但是一直不理解为什么是增加的。今天看了一些参考文献，理解了一点。 对于神经网络，我们会有训练集的数据${x_0, y_0}$，$x$和$y$之间有函数关系$y = f(x)$，函数有自己的参数$p$，对应于神经网络的权值。所以有$y = f(p, x)$。为了能够训练神经网络，让输出和预期值越来越接近，可以定义损失函数(Loss Function)，有$l = L(x_0, y_0, y)$。其中$y = f(p, x_0)$，所以： $$l = L(p, y_0, x_0)$$ 计算$l$关于$p$的梯度，所以： $$\\bigtriangledown{C_{xr}(p)} = &lt; \\frac{\\partial{C_{xr}}}{\\partial{p^{(0)}}}, …, \\frac{\\partial{C_{xr}}}{\\partial{p^{(n)}}}$$ 沿梯度方向，损失函数$l$的值是逐渐增加的 对这句话的理解，在于是什么量沿着梯度方向的变化。应该是自变量$p$。例如： 当$\\frac{\\partial{C_{xr}}}{\\partial{p^{(0)}}}（p_0） &gt; 0$时，也就是说，函数$l(p^{0})$在$p_0$点时，函数曲线沿$p=p^0$的切线斜率是大于0的，也就是说，在很小的一个区间$(p_0-\\delta, p_0+\\delta)$，如果$p_1 &gt; p_0$， 那么有$l(p_1) &gt; l(p_0)$。所以，如果沿着梯度的负方向，损失函数的值也会降低。对于梯度大于0，会比较好理解，因为$l$是增函数。 如果$\\frac{\\partial{C_{xr}}}{\\partial{p^{(0)}}}(p_0) &lt; 0$，那么有$l(p^0)$是减函数，也就是说，函数在$p_0$点沿$p = p^0$的切线斜率是小于0的。即，在很小的一个区间$(p_0-\\delta, p_0+\\delta)$，如果$p_1 &gt; p_0$， 那么有$l(p_1) &lt; l(p_0)$。但是由于梯度本身小于0，所以梯度的反方向就是$p^0$递增的方向。又因为$l(p^0)$是减函数，所以沿梯度的负方向，$l(p^0)$还是会逐渐降低。 Neural Network中梯度下降法的推导这里用最简单的全连接网络为例，如图所示： $x$：网络的输入值 $w_1, w_2, w_3$：层与层的连接参数 $h_1, h_2$：中间层的输入值 $o_1,o_2$：中间层的输出值 $y$：网络的输出值 假设输入参数的个数为$m$，输出参数的个数为$n$，第一层的神经元个数为$a$，第二层的神经元参数为$b$，所以： $x \\in R^{1*m}$ $y \\in R^{1*n}$ $h_1, o_1 \\in R^{1*a}$ $h_2, o_2 \\in R^{1*b}$ $w_1 \\in R^{m*a}$ $w_2 \\in R^{a*b}$ $w_3 \\in R^{b*n}$ 网络中每层的激活函数(activation function)用sigmoid函数： $f(x) = \\frac{1}{1+e^{-x}}$ sigmoid函数的导数有如下特点(可以自己推导)： $f’(x) = f(x)*(1-f(x))$ 假设用来训练的数据集为$&lt;x, r&gt;$，$x$为输入值，$r$为输出值 损失函数为： $L = \\frac{1}{2}*(y-r)^{2}$ 所以有如下公式： $$h_1=w_1*x+b_1$$ $$o_1=sigmoid(h_1)$$ $$h_2=w_2*o_1+b_2$$ $$o_2=sigmoid(h_2)$$ $$h_3=w_3*o_2+b_3$$ $$y=sigmoid(h_3)$$ 计算$L$关于$w_3$的梯度，有： $$\\frac{\\partial{L}}{\\partial{w_3}}=(y-r)*\\frac{\\partial{(y-r)}}{\\partial{w_3}}$$ $$=(y-r)*\\frac{\\partial{(y-r)}}{\\partial{h_3}}*\\frac{\\partial{h_3}}{\\partial{w_3}}$$ $$= (y-r)(y-r)[1- (y-r)]*o_2$$ $$= (y-r)^{2} * (1-y+r) * o_2$$ 类似的，可以得到： $$\\frac{\\partial{L}}{\\partial{b_3}}=(y-r)^{2}*(1-y+r)$$ $$\\frac{\\partial{L}}{\\partial{w_2}}=(y-r)^{2} * (1-y+r) * w_3 * o_2 * (1-o_2) * o_1$$ $$\\frac{\\partial{L}}{\\partial{b_2}}=(y-r)^{2} * (1-y+r) * w_3 * o_2 * (1-o_2)$$ $$\\frac{\\partial{L}}{\\partial{w_1}}=(y-r)^{2} * (1-y+r) * w_3 * o_2 * (1-o_2) * o_1 * (1-o_1) * x$$ $$\\frac{\\partial{L}}{\\partial{b_1}}=(y-r)^{2} * (1-y+r) * w_3 * o_2 * (1-o_2) * o_1 * (1-o_1)$$ 计算损失函数$L$关于网络权重的梯度后，网络权重的变化为： $$\\bigtriangleup W = - \\eta * \\frac{\\partial{L}}{\\partial{W}}$$ 其中， $W$是网络中的权重参数，一般只通过学习率来调节网络训练的快慢，是不够的。会加入动态变化量，以加快学习过程。所以： $$\\bigtriangleup W=-\\eta * \\frac{\\partial{L}}{\\partial{W}} + \\alpha * \\frac{\\partial{L}}{\\partial{W}}$$ 其中，$\\alpha$表示动态变化项，是一个常数。","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankmartinem.github.io/categories/Deep-Learning/"}],"tags":[{"name":"Gradient Descent","slug":"Gradient-Descent","permalink":"http://frankmartinem.github.io/tags/Gradient-Descent/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-02-01T11:50:30.000Z","updated":"2020-02-01T11:50:30.000Z","comments":true,"path":"2020/02/01/hello-world/","link":"","permalink":"http://frankmartinem.github.io/2020/02/01/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files生成静态文件1$ hexo generate More info: Generating Deploy to remote sites将生成的文件部署到github1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Brain Structure Conference","slug":"Brain-Structure-Conference","date":"2020-01-21T06:11:55.000Z","updated":"2021-04-13T00:59:56.263Z","comments":true,"path":"2020/01/21/Brain-Structure-Conference/","link":"","permalink":"http://frankmartinem.github.io/2020/01/21/Brain-Structure-Conference/","excerpt":"","text":"介观脑连接研讨会 Focused Questions 大脑神经回路与功能的关系 大脑神经连接的建模与计算 Muming Poo mapping and understanding genetic programs and neural circuits, subtypes and substates Jun Yan single neuron projectome projection subtypes, CT brain orders and projectome classification, 观察轴突分叉点的夹角 Florian Engert 不同类型的神经元组成不同的神经回路，回路之间互相影响，各有各的作用。 JiuLin Du 神经元分类， vglut2a, vglut2b(excitory), GABA(jinhibotiry)等 神经信号的传递有兴奋和抑制之分，不同的神经递质对应了不同的神经信号的gate gene expression 信息传导通路 Hongkui Zheng multi-level approach to decoding the brain cell types, connectivity, psyology and behavior, modeling and thery 层层递进的网络结构设计 细胞类型的定义，基于细胞类型定义的回路功能研究（对网络结构设计具有一定的参考意义） feedforward and feedback pathway（前向神经网络和反馈式神经网络） corticalcortical, thalamocoritcal and corticalthalamic projection matrices(Harris, Nature, 2019) sparse labeling（稀疏标签） shared types of neurons in different cortical areas Chenyu Li causal map, behavior, memory in the brain connectome without function is meanless Linda Richard Dan Yang David Van Essen resting-state networks XiaoJing Wang Theoretical Neuroscience Rising Computational and Congnitive Neuroscience(CCN) generative and connectivity connectivity is insufficient to predict dynamics Helen Barbas Anthony Movshon brain models and simulation mainflold","categories":[{"name":"Lecture","slug":"Lecture","permalink":"http://frankmartinem.github.io/categories/Lecture/"}],"tags":[]},{"title":"Unscented Kalman Filter","slug":"Unscented-Kalman-Filter","date":"2020-01-21T06:10:25.000Z","updated":"2021-04-13T01:02:58.696Z","comments":true,"path":"2020/01/21/Unscented-Kalman-Filter/","link":"","permalink":"http://frankmartinem.github.io/2020/01/21/Unscented-Kalman-Filter/","excerpt":"","text":"Unscented Kalman Filter最近读了一篇文献，里面用到了无迹卡尔曼滤波(Unscented Kalman Filter)。这里写一下我对这种方法的理解。卡尔曼滤波的理解部分可以参考 我的一点点理解无迹卡尔曼滤波是对卡尔曼滤波的一种改进。这种改进主要是针对非线性的信号。因为在卡尔曼滤波中，预测模型以及测量空间对应的转换矩阵都是都是线性转换。但是在面对非线性信号时，会出现无法拟合的情况。所以就有了无极卡尔曼滤波。这种方法的主要改进在于，不再用线性的模型去计算预测模型以及转换矩阵，而是通过采样和计算均值方法的方式，去估计样本的方差和均值。 计算过程无迹卡尔曼滤波的计算方式和卡尔曼滤波比较类似，只是讲线性转换模型换成了采样的方式。具体的原理推导比较复杂，所以这里只写一下无迹卡尔曼滤波的计算过程： 无迹卡尔曼的计算步骤和卡尔曼滤波基本是一致的，只是对其中的一些步骤进行了修改，首先，我们看一下Kalman Filter的计算过程： 建立编码模型和转换模型， 假设观测变量是$z$， 测量变量是$x$， 那么首先我们假设： 当前时刻的测量变量是可以根据上一时刻的测量变量估计：$$x_{t} = Fx_{t-1} + w_t, (w_t -N(0, Q))$$ 当前时刻的观测变量可以根据测量变量估计： $$ z_t = Hx_t + r_t, (r_t - N(0, R)) $$ 根据以上的编码模型和转换模型，Kalman Filter的计算流程如下： 首先，根据已知的模型，以及上一时刻的卡尔曼估计值，计算当前时刻的模型预测值 $$x_t’=Fx_{t-1}$$ 根据当前的模型预测值，计算对应的协方差 $$P(x_t|x_t’)=FP(x_t|X_t)F^T$$ 根据当前的协方差和测量空间的转换矩阵，计算当前时刻的卡尔曼增益 $$K_t=P(x_t|x_t’)H^T(HP(x_t|x_t’)H^T+R)^{-1}$$ 根据卡尔曼增益和测量值，计算当前时刻的卡尔曼估计值 $$x_t=x_t’+K_t(z_t-Hx_t’)$$ 计算了当前时刻的卡尔曼估计值之后，还需要计算当前的估计值和真实值的协方差矩阵，方便下一次计算 $$P(x_t|X_t)=(I-HK_t)P(x_t|x_t’)$$ 作为线性的解码器，Kalman Filter确实能找到观测变量和测量变量之间的关系，并用观测变量去纠正当前测量变量中的误差。但是涉及到非线性关系的时候，Kalman Filter的线性假设就不成立了。这时有两种优化的方法： 如果已知这种非线性关系的公式，例如加速度和位置的关系等，那么可以把上述转换模型和观测模型换成已知的非线性模型，增加解码准确率。这种方法就是**扩展卡尔曼滤波(Extend Kalman Filter)**。这种方法的优点在于拟合更加准确，但是缺点也很明显。首先是计算量增加，如果非线性拟合涉及很复杂的模型，那么计算量比Kalman Filter增加很多。然后是非线性模型，并不是任何时候，这种模型都是已知的，如果不是已知的，那就需要进行非线性拟合，找到最合适的拟合模型，例如指数模型，高阶模型等，再次增加计算量。 如果不知道这种非线性关系的公式，那么我们可以进行非线性拟合或者直接假设一个公式。但是我们观察Kalman Filter的计算过程，整个估计过程中，用到了当前时刻的值，以及协方差。而这两个量，我们是能通过采样的方式得到的，即，可以不需要直接计算非线性模型的协方差矩阵，直接通过采样估计，类似蒙特卡洛的方法。但是采样的计算量会更大，因为需要大样本才能得到准确的估计。目前有另外一种办法，能够用很少的采样点(几个)就得到准确的估计，这种方法是无迹变换(Unscented Transform)，结合到Kalman Filter中，就是无迹卡尔曼滤波(Unscented Kalman Filter) 所以无迹卡尔曼滤波的主要流程如下： 计算转换模型和编码模型 建立转换模型，可以是非线性也可以是线性，这里用线性模型：$$x_{t} = Fx_{t-1} + w_t, (w_t -N(0, Q))$$ 建立编码模型，也可以是线性或非线性模型：$$z_t = Hx_t + r_t, (r_t - N(0, R))$$ 根据上述模型和训练集数据，用最小二乘法或其他的拟合方法，得到模型参数，然后开始无迹卡尔曼的预测和更新阶段 根据模型预测$x_{t}$ $$ x_t’=Fx_{t-1} $$ 预测$x_{t}$的协方差 $$ P(x_t|x_t’)=FP(x_t|X_t)F^T + Q $$ 用采样点估计当前协方差矩阵，先采样$2d+1$个点，并保证中心点的值为$x_t’$ $$ X_0 = x_t’ $$ $$ X_i = x_t’ + (\\sqrt{(d + k)P(x_t|x_t’)})_{i}, i = 1, …, d$$ $$ X_i = x_t’ - (\\sqrt{(d + k)P(x_t|x_t’)})_{i}, i = d + 1, …, 2d$$ 计算采样点的权重值 $$ w_0= \\frac{k}{d+k}, w_i = \\frac{1}{2d+k}, i = 1, … 2d $$ 根据转换矩阵，采样点，计算观测值和测量值的关系$$Z_i = h(X_i), i = 0, …2d$$ $$z_t = \\sum_{i = 0, …2d}{w_{i}Z_{i}}$$ 根据采样点估计的观测值，计算观测值$z$的方差，以及观测值$z$和测量值$x$的协方差$$P_{zz, t} = w_{0}(Z_{0}-z_{t})(Z_{0}-z_{t})^T + (\\sum_{i=1, …,2d}{w_{i}(Z_{i}-Z_{0})(Z_{i}-z_{0})^T}) + R$$ $$P_{xz, t} = w_{0}(Z_{0}-z_{t})(Z_{0}-z_{t})^T + (\\sum_{i=1, …, 2d}{w_{i}(X_{i}-X_{0})(Z_{i}-Z{0})^T})$$ 根据计算的协方差，可以计算Kalman增益$$K = P_{xz, t}P_{zz, t}^{-1}$$ 用Kalman增益计算最有估计值$$x_t = x_t’ + K_t(h(x_t’)-z_t)$$ $$P(x_t|X_t) = P(x_t|x_t’)-P_{xz, t}(P_{zz, t}^{-1})^TP_{xz, t}^{T}$$ 以上就是无迹卡尔曼滤波的主要步骤，后续会附上代码链接","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankmartinem.github.io/categories/Algorithm/"}],"tags":[{"name":"Unscented Kalman Filter","slug":"Unscented-Kalman-Filter","permalink":"http://frankmartinem.github.io/tags/Unscented-Kalman-Filter/"}]},{"title":"Unity","slug":"Unity","date":"2020-01-21T06:10:10.000Z","updated":"2021-04-13T01:02:52.787Z","comments":true,"path":"2020/01/21/Unity/","link":"","permalink":"http://frankmartinem.github.io/2020/01/21/Unity/","excerpt":"","text":"Unity相关概念 ConfigurableJoint OnEnable()函数在gameObject.setActive(true)时触发，优先于Start()，但是和Awake()函数的先后顺序不确定；OnDisable()函数在gameObject.setActive(false)时触发 Lerp() 计算两个点之间的插值，函数如下： 1Lerp(Vector3 a, Vector3 b, float t); 计算公式如下： $$ (b - a) * t $$ Mathf.Approximately() 判断两个浮点数是否十分接近，使用方法如下： 1Mathf.Approximately(float a, float b); 返回值为bool Quaternion类 Quaternion属于四元数，包括x, y, z, w四个分量，和欧拉角一样，是3D图形中常用的坐标变换表示方法之一，对于插值，平滑以及数据存储，都有较大的优势（相较于传统的矩阵表示方法）","categories":[{"name":"coding","slug":"coding","permalink":"http://frankmartinem.github.io/categories/coding/"}],"tags":[{"name":"Unity","slug":"Unity","permalink":"http://frankmartinem.github.io/tags/Unity/"}]},{"title":"Two Types of Variance","slug":"Two-Types-of-Variance","date":"2020-01-21T06:09:44.000Z","updated":"2021-04-13T01:02:43.934Z","comments":true,"path":"2020/01/21/Two-Types-of-Variance/","link":"","permalink":"http://frankmartinem.github.io/2020/01/21/Two-Types-of-Variance/","excerpt":"","text":"样本方差和统计方差我们知道，统计学上方差的计算公式如下： $$ \\sigma^2=\\frac{\\sum_{i=1}^{n}(x_i-\\mu)}{n}$$ 这是统计学中方差的定义，已知条件有总体的均值$\\mu$，以及总体个数$n$，公式的另一种写法为： $$\\sigma^2=E[(x-\\mu)^2]=\\sum{(x-\\mu)^2}p(x)$$ 其中$p(x)$是$x$出现的概率，所以这个公式只对于离散变量有效。 那么，如果总体量很大，不能做到全部采样，那么就需要用样本来估计总体，假设从总体为$N$的总数中抽取$n$个样本，其中$(N&gt;&gt;n)$，采样值为$x_1,x_2,…,x_n$ 样本均值为： $$\\bar{x}=\\frac{\\sum_{i=1}^{n}{x_i}}{n}$$ 样本的方差为： $$ S^2=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})}{n}$$ 但是样本的方差和总体的方差是有差别的，计算样本方差的期望值，来估计样本方差和实际方差$\\sigma^2$之间差了多少： $$ E[S^2]=E[\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})}{n}]$$ $$=E[\\frac{1}{n}\\sum_{i=1}^{n}{((x_i-\\mu)-(\\bar{x}-\\mu))^2}]$$ $$=E[\\frac{1}{n}\\sum_{i=1}^{n}{((x_i-\\mu)^2-2(x_i-\\mu)(\\bar{x}-\\mu)+(\\bar{x}-\\mu)^2)}]$$ $$=E[\\frac{1}{n}\\sum_{i=1}^{n}{(x_i-\\mu)^2}-\\frac{2}{n}(\\bar{x}-\\mu)\\sum_{i=1}^{n}{(x_i-\\mu)}+(\\bar{x}-\\mu)^2]$$ 其中 $\\sum_{i=1}^{n}{(x_i-\\mu)}$ $=\\sum_{i=1}^{n}{x_i}-\\sum_{i=1}^{n}{\\mu}$ $=n(\\bar{x}-\\mu)$ 所以 $=E[\\frac{1}{n}\\sum_{i=1}^{n}{(x_i-\\mu)^2}-\\frac{2}{n}(\\bar{x}-\\mu)\\sum_{i=1}^{n}{(x_i-\\mu)}+(\\bar{x}-\\mu)^2]$ $=E[\\frac{1}{n}\\sum_{i=1}^{n}{(x_i-\\mu)^2}-2(\\bar{x}-\\mu)^2+(\\bar{x}-\\mu)^2]$ $=\\sigma^2-E[(\\bar{x}-\\mu)^2]$ （这里$\\sigma^2$是因为样本方差的期望值是总体方差） $E[(\\bar{x}-\\mu)^2]$ $=E(\\bar{x}-E[\\bar{x}])^2$ $=var(\\bar{x})$ $=\\frac{1}{n^2}var(\\sum_{i=1}^{n}{x_i})$ $=\\frac{1}{n^2}\\sum_{i=1}^{n}{var(x_i)}$ $=\\frac{n\\sigma^2}{n^2}$ $=\\frac{\\sigma^2}{n}$ 根据上面推导的式子，有以下计算： $\\sigma^2-E[(\\bar{x}-\\mu)^2]$ $=\\sigma^2-\\frac{\\sigma^2}{n}$ $=\\frac{n-1}{n}\\sigma^2$ 也就是说，样本估计的方差是总体方差的$\\frac{n-1}{n}$倍，即所谓的有偏估计。要转换成无偏估计，只需要乘以倍数就可以了 $$\\frac{n}{n-1}S^2=\\frac{n}{n-1}\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})}{n}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})}{n-1}$$ 这即是所谓的无偏估计。 当然，还有一种比较直接的解释，由于是求统计样本中的方差，所以在求解统计样本均值时，已经用掉了一个自由度的值，所以求方差时，其实有用的值会少一个。例如在只有一个样本时，这时求方差是没有意义的。不过在概率论中，求此时的方差是有意义的，因为已经知道了总体的概率分布，所以即使只有一个样本，总体的分布是不变的。其中区别就在于统计样本只是用于估计。","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"http://frankmartinem.github.io/categories/Mathematics/"}],"tags":[{"name":"Statistics","slug":"Statistics","permalink":"http://frankmartinem.github.io/tags/Statistics/"}]},{"title":"Population Vector Algorithm","slug":"Population-Vector-Algorithm","date":"2020-01-21T06:09:04.000Z","updated":"2021-04-13T01:02:17.351Z","comments":true,"path":"2020/01/21/Population-Vector-Algorithm/","link":"","permalink":"http://frankmartinem.github.io/2020/01/21/Population-Vector-Algorithm/","excerpt":"","text":"PVA(Population Vector Algorithm)算法推导： 信号预处理 这里的算法推导主要针对神经元集群解码，因为PVA的主要应用还是在神经元解码中 首先，采集到的spike信号是以发放次数的方式存储的，这里需要先转换成发放率的形式，即： $$fr[n]=\\frac{spk[n]}{\\Delta t} \\tag{1}$$ 其中，$fr[n]$表示$n$时刻神经元的发放率，$\\Delta t$表示一个bin的长度，通常的取值为20ms，30ms，50ms，1000ms等。$spk[n]$表示神经元在第$n$个bin中发放的次数。 然后，对发放率做一个FIR滤波，主要目的是平滑发放率曲线，计算公式如下： $$s[n]=\\sum_{i=1}^{W-1}{fr[n-i]h[i]} \\tag{2}$$ 其中，$h[i]$表示滤波器的卷积函数，可以根据需求选取，$W$表示滤波器的阶数，可以根据实际需要选择。 PVA算法原理 PVA算法的提出，主要是根据实验中观察到的现象。在猴子将手臂移动向不同的方向时，不同的神经元发放的率产生了变化，我们由此假设，神经元的发放率跟运动方向是有关系的，所以我们想到，用余弦曲线的方式，去拟合神经元的发放率与运动方向之间的关系。首先，我们假设每个神经元都有一个自己的偏好方向$\\theta_{PD}$，假设此时，猴子手臂的运动方向为$\\theta$，那么此时神经元的发放率为： $$f=m*cos(\\theta-\\theta_{PD})+b_0 \\tag{3}$$ 其中，$m$为表征神经元活泼性的参数，即有的神经元可能表征的偏好方向一样，但是在偏好方向上的发放率变化是不一样的。$b_0$表示神经元的基础发放率，即在静息状态下的基础发放率。$f$表示的是神经元在猴子手臂朝向$\\theta$方向运动时的发放率，注意这里是发放率不是spike count，虽然两者可以通过bin转换，但是公式推导的时候两者还是不一样的。 公式$(3)$表示了单个神经元的发放与运动的关系。猴子大脑M1区域的神经元是很多的，对不同的方向肯定有不同的偏好性。那么如何处理这种不一致性呢，我们的方法是用矢量求和的形式，得出一个此时最可能的运动方向。即： $$\\vec{u}=\\frac{1}{N} \\sum_{i=1}^{n}{m*cos{\\theta_{PD}}} \\tag{4}$$ 这里$\\vec{u}$表示神经元此时解码出来的运动方向，这里也能部分表征运动速度，但是速度的大小也与实际的运动距离有关，所以，运动速度的计算如下： $$v=k*\\vec{u}+\\sigma \\tag{5}$$ 这里$k$表示实际速度与计算得出的速度的比例，$\\sigma$表示实际速度与解码得到的速度之间的误差，以上就是PVA算法的主要原理 3. 参数计算 那么，现在的问题在于，如何计算PVA算法中的几个参数，这里我们用最小二乘法的方式，求最小误差情况下的参数$b_0,m,\\theta_{PD}$，我们将公式$(3)$换一种写法，即： $$f = b_0 + b_1 * cos \\theta + b_2 * sin \\theta \\tag{6}$$ 再考虑$cos{\\theta}$和$sin{\\theta}$这两个量，对应在速度中，可以表示为归一化过后的$v_x$和$v_y$，只要在$[-1,1]$这个区间内所以，将公式$(6)$写成： $$f = b_0 + b_1 * v_x + b_2 * v_y \\tag{7}$$ 用最小二乘法计算，误差为： $$\\epsilon = \\sum_{ i=1 }^{n}{(b_0 + b_1 * v_x + b_2 * v_y - f)^2} \\tag{8}$$ 最终计算结果根据要推导一下，这里先暂时不写，回去再补充 所以，极值在偏导数为$0$的地方取得，即： $$\\frac{\\partial{\\epsilon}}{\\partial{b_0}}=0 \\tag{9}$$ $$\\frac{\\partial{\\epsilon}}{\\partial{b_1}}=0 \\tag{10}$$ $$\\frac{\\partial{\\epsilon}}{\\partial{b_2}}=0 \\tag{11}$$ 解上述方程，可以得到$b_0,b_1,b_2$的值，即： $$\\beta=(A^T*A)^{-1}A^TB \\tag{12}$$ 其中，$\\beta=(b_0,b_1,b_2)$，$A$为运动信息矩阵，$B$为神经信号矩阵。","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankmartinem.github.io/categories/Algorithm/"}],"tags":[{"name":"PVA","slug":"PVA","permalink":"http://frankmartinem.github.io/tags/PVA/"}]},{"title":"Hidden Markov Model","slug":"Hidden-Markov-Model","date":"2020-01-21T06:08:45.000Z","updated":"2021-07-24T17:31:03.506Z","comments":true,"path":"2020/01/21/Hidden-Markov-Model/","link":"","permalink":"http://frankmartinem.github.io/2020/01/21/Hidden-Markov-Model/","excerpt":"","text":"Hidden Markov Model 背景 隐马尔可夫模型(Hidden Markov Model)是一种常用的统计模型。应用也比较广泛，在时序问题，以及语音识别等问题上有广泛的应用。下面简单介绍一下隐马尔可夫模型。 隐马尔可夫模型是在马尔可夫过程的基础上，加入了隐含状态后的一种结构。这里首先介绍一下什么是马尔可夫过程(Markov Process) 在一个随机过程中，有一个状态变量III，其下一时刻的状态之和之前的状态有关。例如布朗运动，粒子的下一时刻状态之和之前时刻的状态有关。而III变化的过程，也就是马尔科夫链。这个约束，也就是马尔可夫假设。 在马尔可夫过程中，模型还是很复杂，我们还可以加约束来让模型变得简单一点。我们可以假设，状态变量III的下一时刻状态只和上一时刻的状态有关。这样就得到了齐次马尔可夫模型。即： p(It∣It−1,It−2,...,I0)=p(It∣It−1),t=1,2,...,Tp(I_t|I_{t-1}, I_{t-2}, ..., I_{0}) = p(I_t|I_{t-1}), t=1, 2, ..., T p(I​t​​∣I​t−1​​,I​t−2​​,...,I​0​​)=p(I​t​​∣I​t−1​​),t=1,2,...,T 我们可以看出，马尔可夫模型的描述，只针对某一个变量而言。但是实际生活中，很多变量之间都是相关的。例如你的运动是由肌肉的收缩和舒张来完成的。但是在观察者看来，你只是完成了一个简单的运动。其中，你的运动状态就是观测到的变化量，而肌肉的状态就是隐藏的状态。所以HMM模型的结构如下图所示： 和马尔可夫过程一样，HMM也有一些约束条件。首先，HMM要满足马尔可夫假设且满足齐次马尔可夫模型，即： p(It∣It−1,ot−1,...,I0,o0)=p(It∣It−1),t=1,2,...,Tp(I_t|I_{t-1}, o_{t-1}, ..., I_{0}, o_{0}) = p(I_t|I_{t-1}), t=1, 2, ..., T p(I​t​​∣I​t−1​​,o​t−1​​,...,I​0​​,o​0​​)=p(I​t​​∣I​t−1​​),t=1,2,...,T 然后是观测独立性假设，也就是说任意时刻的观测值只依赖于当前时刻的马尔可夫链的状态iti_ti​t​​， 即： p(ot∣It,It−1,ot−1,...,I0,o0)=p(ot∣It),t=1,2,...,Tp(o_t|I_t, I_{t-1}, o_{t-1}, ..., I_{0}, o_{0}) = p(o_t|I_t), t=1, 2, ..., T p(o​t​​∣I​t​​,I​t−1​​,o​t−1​​,...,I​0​​,o​0​​)=p(o​t​​∣I​t​​),t=1,2,...,T 原理 HMM的结构如上图所示，其中III是状态变量，OOO是观测变量。假设QQQ是所有可能的状态的集合，VVV是所有可能的观测的集合。 Q={q1,q2,...,qN}Q = \\{ q_1,q_2,...,q_N \\} Q={q​1​​,q​2​​,...,q​N​​} V={v1,v2,...,vM}V = \\{v_1,v_2,...,v_M \\} V={v​1​​,v​2​​,...,v​M​​} 即可能的状态有N种， 可能的观测值有M种，两者不一定会相等。那么在一次试验中，观测到的值为OOO，每个观测值会唯一对应一个状态值，因为试验已经结束了，假设状态序列为III，那么OOO和III的长度一样，假设为T，那么： $$O = { O_1,O_2,…,O_T }$$ I={I1,I2,...,IT}I = \\{ I_1,I_2,...,I_T \\} I={I​1​​,I​2​​,...,I​T​​} 在ttt时刻会有一个状态值，那么下一个时刻的状态值会与上一时刻相关，当然也可以是不相关的，由此给出状态矩阵AAA的定义： A=[aij]A=[a_{ij}] A=[a​ij​​] aija_{ij}a​ij​​表示当前时刻ttt状态为qiq_iq​i​​的情况下，下一时刻的状态为qjq_jq​j​​的概率，这里i,j=1,2,...Ni,j=1,2,...Ni,j=1,2,...N，用数学形式表示，即： $$a_{ij}=P(I_{t+1}=q_j | I_t=q_i)$$ 有了状态转移矩阵后，我们并不能直接估计下一时刻的状态，因为状态在整个试验过程中是隐藏的，试验中只能得到观测值的相关信息，所以还要有观测值和状态值之间的转换矩阵，即当观测到某个值时，其对应于各个状态的概率分别是多少。假设观测概率矩阵是BBB，给出BBB的定义： B=[bjk]B=[b_{jk}] B=[b​jk​​] bjkb_{jk}b​jk​​表示当前时刻ttt状态值为qjq_jq​j​​的情况下，观测值为vkv_kv​k​​的概率。所以有k=1,2,...Mk=1,2,...Mk=1,2,...M，j=1,2,...,Nj=1,2,...,Nj=1,2,...,N，用数学形式表示，即： bjk=P(ot=vk∣it=qj)b_{jk}=P(o_t=v_k | i_t=q_j) b​jk​​=P(o​t​​=v​k​​∣i​t​​=q​j​​) 确定了观测值和状态值之间的转换概率，当前时刻和下一时刻之间的状态转换概率，那么我们还需要确定可能的观测值在试验刚开始时被选中的概率，假设为π\\piπ，给出π\\piπ的定义： π=[πi]\\pi=[\\pi_{i}] π=[π​i​​] 其中πi\\pi_{i}π​i​​表示观测值qiq_iq​i​​在刚开始被选中的概率，那么，i=1,2,...,Ni=1,2,...,Ni=1,2,...,N，用数学的形式表示，即： πi=P(I1=qi)\\pi_i=P(I_1=q_i) π​i​​=P(I​1​​=q​i​​) 到这里，整个HMM模型中的主要参数已经全部介绍了，由介绍可知，根据π,A,B\\pi,A,Bπ,A,B可以让一个HMM模型顺利工作。可以求出在任意状态序列对应的概率P(O∣λ)P(O|\\lambda)P(O∣λ)。所以，我们也用这些参数来表示一个HMM模型，即： $$\\lambda={ A,B,\\pi }$$ 。 常见问题 以上介绍了HMM的基本概念，在实际应用中，主要有以下几个基本问题： 已知模型λ\\lambdaλ以及观测序列OOO，计算在这种模型下出现这种观测序列的概率P(O∣λ)P(O|\\lambda)P(O∣λ) 已知观测序列OOO，但是不知道模型λ\\lambdaλ，计算模型λ\\lambdaλ，使得当前观测序列产生的概率P(O∣λ)P(O|\\lambda)P(O∣λ)最大 给定模型λ\\lambdaλ和观测序列OOO，计算最有可能产生这一观测序列的状态序列III，即使得P(I|O,\\lambda)最大的III 以上就是最常见的HMM问题，主要涉及到模型中各个参数计算的问题。 在问题１中，我们需要计算观测序列出现的概率，主要可以用来判断出现的这一观测序列是否常见，如果计算得到的概率很低，但是在实际观测中却经常出现，那么就需要检查系统中是否出现了外部干扰。 在问题2中，我们需要计算模型的参数。主要是用于模型的学习和自适应参数调整的问题。模型是不确定的，但是根据给定的观测序列，我们需要找到一个最合适的模型，来保证出现这一观测序列的概率最大。有点类似回归求最优解或者神经网络拟合的思想。 在问题3中，我们需要通过观测序列和模型，来估计隐藏状态。这个主要适用于一些解码问题。通过观测值求解隐藏值。 针对以上的问题，分别有对应的解决办法。下面会介绍最常见的一些解法。当然，由于ＨＭＭ中，观测变量和隐藏状态可能的取值是有限的。所以其实用穷举法也可以算，只是计算量会很大。 解决办法 问题1 已知模型和观测序列，要计算出现这种观测序列的概率P(O∣λ)P(O|\\lambda)P(O∣λ) 这个问题有两种解法，前向和后向算法。两种方法比较类似。 前向算法 首先，我们定义一个概率： pt(i)=P(o1,o2,...,ot,It=qi)p_t(i) = P(o_1, o_2, ..., o_t, I_t=q_i) p​t​​(i)=P(o​1​​,o​2​​,...,o​t​​,I​t​​=q​i​​) pt(i)p_t(i)p​t​​(i)表示观测序列为o0,o1,...,ot{o_0, o_1, ...,o_t}o​0​​,o​1​​,...,o​t​​，同时It=qiI_t=q_iI​t​​=q​i​​的概率。所以我们有以下递推公式： pt(i)=(∑j=1Npt−1(j)aji)bikp_{t}(i) = (\\sum_{j=1}^{N}p_{t-1}(j)a_{ji})b_{ik} p​t​​(i)=(​j=1​∑​N​​p​t−1​​(j)a​ji​​)b​ik​​ 同时，有ot=vko_{t}=v_{k}o​t​​=v​k​​。在上面的公式中，∑j=0Npt−1(j)aji\\sum_{j=0}^{N}p_{t-1}(j)a_{ji}∑​j=0​N​​p​t−1​​(j)a​ji​​表示前t−1t-1t−1个输出为o1,o2,...,ot−1{o_1, o_2, ..., o_{t-1}}o​1​​,o​2​​,...,o​t−1​​，且第ttt个隐藏状态为qiq_iq​i​​的概率。因为t−1t-1t−1时刻的状态是任何值都可以，只需要乘以对应的转移概率，就可以计算出ttt时刻状态为qiq_iq​i​​的概率了。 然后在初始状态时，有： p1(i)=πibik,o1=vkp_1(i) = \\pi_ib_{ik}, o_1=v_k p​1​​(i)=π​i​​b​ik​​,o​1​​=v​k​​ 所以最终得到的概率为： P(O∣λ)=∑i=1NpT(i)P(O|\\lambda) = \\sum_{i=1}^{N}p_T(i) P(O∣λ)=​i=1​∑​N​​p​T​​(i) 也就是说，在TTT时刻，观测序列为o1,o2,...,oT{o_1, o_2, ..., o_T}o​1​​,o​2​​,...,o​T​​，且模型为λ\\lambdaλ的概率为观测序列为o1,o2,...,oT{o_1, o_2, ..., o_T}o​1​​,o​2​​,...,o​T​​且TTT时刻状态值为q1,q2,...,qN{q_1, q_2, ..., q_N}q​1​​,q​2​​,...,q​N​​的所有值的和。 后向算法 后向算法和前向算法比较类似，都是通过递推的方式逐步计算观测序列的概率。不同的地方是，后向算法是从后往前算，前向算法是从前往后算。 假设观测序列的长度为TTT，并定义从t+1t+1t+1时刻到TTT时刻的序列为ot+1,ot+2,...,oT{o_{t+1}, o_{t+2}, ..., o_T}o​t+1​​,o​t+2​​,...,o​T​​，且ttt时刻的隐藏状态为qiq_iq​i​​的概率为： pt(i)=P(ot+1,ot+2,...,oT,It=qi∣λ)p_t(i) = P(o_{t+1}, o_{t+2}, ..., o_T, I_t=q_i|\\lambda) p​t​​(i)=P(o​t+1​​,o​t+2​​,...,o​T​​,I​t​​=q​i​​∣λ) 对于后向算法，初始状态应该是pT(i)p_T(i)p​T​​(i)，表示的是观测序列为oT+1{o_{T+1}}o​T+1​​时，且隐藏状态为qiq_iq​i​​的概率，但是因为已经知道了oTo_To​T​​的状态了，且oT+1o_{T+1}o​T+1​​并没有发生，所以这里其实给任意值都可以。这个值其实主要表示的是T+1T+1T+1时刻和TTT时刻的关系，但是这个关系并不知道，所以给任意值都是可以的。表示这个关系可以是任意的。 然后和前向算法类似，我们可以计算后向的递推公式： pt(i)=∑j=1Naijbjkpt+1(j)p_t(i) = \\sum_{j=1}^{N}a_{ij}b_{jk}p_{t+1}(j) p​t​​(i)=​j=1​∑​N​​a​ij​​b​jk​​p​t+1​​(j) 其中有，ot+1=vko_{t+1} = v_ko​t+1​​=v​k​​ ∑j=1Naijpt+1(j)\\sum_{j=1}^{N}a_{ij}p_{t+1}(j)∑​j=1​N​​a​ij​​p​t+1​​(j)表示t+2t+2t+2时刻状态为qjq_jq​j​​且ttt时刻的状态为qiq_iq​i​​的所有可能的t+2t+2t+2时刻的值的和，所以aijbjkpt+1(j)a_{ij}b_{jk}p_{t+1}(j)a​ij​​b​jk​​p​t+1​​(j)表示的是，t+1t+1t+1时刻的观测值为ot+1o_{t+1}o​t+1​​，也就是vkv_kv​k​​，同时t+1t+1t+1时刻的状态值为qjq_jq​j​​的概率。求和之后就是，t+1t+1t+1到TTT时刻的观测值为ot+1,ot+2,...,oT{o_{t+1}, o_{t+2}, ..., o_{T}}o​t+1​​,o​t+2​​,...,o​T​​，且$ t时刻的隐藏状态为qiq_iq​i​​的概率。也就是pt(i)p_t(i)p​t​​(i)。 所以可以得到，最终计算的概率为： P(O∣λ)=∑i=1Nπibi(o1)p1(i)P(O|\\lambda) = \\sum_{i=1}^{N}\\pi_{i}b_i(o_1)p_1(i) P(O∣λ)=​i=1​∑​N​​π​i​​b​i​​(o​1​​)p​1​​(i) 其中，p1(i)p_1(i)p​1​​(i)表示的是观测序列为o2,o3,...,oT{o_2, o_3, ..., o_T}o​2​​,o​3​​,...,o​T​​，$ b_i(o_1)p_1(i)表示观测序列为{o_1, o_2, …, o_T}。所以\\pi_ib_i(o_i)p_1(i)表示观测序列为o_1, o_2, …, o_T, 且I_1=q_i的概率，对所有的I_1={q_1, q_2, …, q_N}求和，就是观测序列为{o_1, o_2, …, o_N}$的概率 以上就是两种计算观测序列概率的算法。主要的思想都是通过递推计算。 问题2 已知观测序列OOO， 计算使得P(O∣λ)P(O|\\lambda)P(O∣λ)最大的模型参数λ\\lambdaλ 这个问题有点类似于回归问题中的拉格朗日极值问题，但是由于涉及到隐藏变量的极大似然估计，所以这里并不能用求导的方法来计算。广泛使用的一种计算方法是EM(Expectation Maximum)算法。关于EM算法，会在后续的文章中介绍，这里暂且不写。 问题3 已知观测序列OOO和模型参数λ\\lambdaλ，求可能产生这一观测序列的隐藏状态III, 使得P(I∣λ)P(I|\\lambda)P(I∣λ)最大 这个问题类似于常见的解码问题。对于HMM模型下的解码问题，一般是用动态规划的方法来求解的。因为这样计算量会降低。常用的HMM解码问题的解决办法是维特比算法(Viterbi Algorithm)。这个也会在后续的文章中介绍。这里暂且不写。","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankmartinem.github.io/categories/Algorithm/"}],"tags":[{"name":"Hidden Markov Model","slug":"Hidden-Markov-Model","permalink":"http://frankmartinem.github.io/tags/Hidden-Markov-Model/"}]},{"title":"Dimensionality Reduction","slug":"Dimensionality-Reduction","date":"2020-01-21T06:08:27.000Z","updated":"2021-07-24T17:35:22.995Z","comments":true,"path":"2020/01/21/Dimensionality-Reduction/","link":"","permalink":"http://frankmartinem.github.io/2020/01/21/Dimensionality-Reduction/","excerpt":"","text":"关于几种降维算法 写一下几种主要降维算法的基本原理和实现 PCA(Principle Component Analysis) PCA是最常见的一种降维算法，其核心思想是数据从高维到低维的投影，使其方差最大化。这个也很好理解，比如，这里我们假设有3组数据a1,a2,a3a_1,a_2,a_3a​1​​,a​2​​,a​3​​，然后第1组的值可以用第2组数据的函数表示，比如a2=2∗a1a_2=2*a_1a​2​​=2∗a​1​​。如果以a1,a2,a3a_1,a_2,a_3a​1​​,a​2​​,a​3​​为坐标画出对应的图像，那么在3维空间中就对应了一个平面，以这个平面的坐标轴为参数，此时看到的就是二维数据，相当于降维了。 插图（参考文献） 参考文献： 假设我们有mmm组nnn维数据，希望能降维到kkk维(k&lt;dk&lt;dk&lt;d)，PCA的计算过程如下： 数据零均值化 求协方差矩阵 求协方差矩阵对应的特征值和特征向量 将特征向量按特征值大小取前kkk行从上向下组成矩阵 将得到的矩阵乘以XXX就能得到降维后的数据 这里数据零均值化主要是为了方便后面的计算（测试一下不做这一步有什么问题） 然后求协方差矩阵，之所以选择协方差矩阵，是因为协方差能很好地反应不同维度之间的差异，假设数据集为X=x1,x2,...,xm∈Rm∗nX={x_1,x_2,...,x_m}\\in R^{m*n}X=x​1​​,x​2​​,...,x​m​​∈R​m∗n​​， 那么协方差矩阵CovCovCov的定义为 CovX(i,j)=∑xixj1mCovX(i,j)=\\sum_{} x_ix_j\\frac{1}{m} CovX(i,j)=​​∑​​x​i​​x​j​​​m​​1​​ 因为之前做过零均值化，所以这里xix_ix​i​​和xjx_jx​j​​的均值都是0。 可以看出，协方差矩阵非对角线上的值表示了不同维度上数据之间的差异，对角线上的数据表示了每个维度的数据分布的差异。即在所有组数据中，每个维度上的变化大小的评价。对于协方差矩阵，当CovX(i,j)=0CovX(i,j)=0CovX(i,j)=0时，说明第iii维和第jjj维的数据是相互独立的，所以，PCA优化的目标在于，尽可能让不同维度之间的协方差为0，而尽可能增大维度自身的方差。 关于求协方差矩阵的特征值，可以理解为将一个特征向量在nnn维空间中进行旋转和拉伸变换，使之与特征向量自己在同一直线上并成一定的比例，那么这个变换就是这个矩阵（参考二维情况下，二维平面中对向量的拉伸和旋转都可以通过一个二阶方阵来实现，高维空间中同理），而这个比例就是特征值。在nnn维空间中，这样的特征向量最多有nnn个，这个可以参考特征向量的求法，当转换成方程组之后，nnn个方程组最多只能有nnn组解。关于为什么要求矩阵的特征值和特征向量，这是根据优化问题的解得到的。假设降维后的矩阵为Y∈Rm∗kY \\in R^{m*k}Y∈R​m∗k​​，转换矩阵为TTT，那么YYY的协方差为 CovY=Y∗YT∗1mCovY=Y * Y^T * \\frac{1}{m} CovY=Y∗Y​T​​∗​m​​1​​ =(TY)∗(TY)T∗1m=(TY) * (TY)^T * \\frac{1}{m} =(TY)∗(TY)​T​​∗​m​​1​​ =T∗CovX∗TT=T * CovX * T^T =T∗CovX∗T​T​​ 所以对于转换矩阵TTT，我们需要通过计算后，使得CovYCovYCovY为一个对角矩阵，并且矩阵中的值依次从大到小排列，因为根据优化的目标，我们需要使CovYCovYCovY的对角线上的值最大，且除对角线以外的数都为0。我们知道，实对称矩阵的不同特征向量是正交的。所以将CovXCovXCovX进行特征分解求出特征值和特征向量，然后取前kkk组特征向量组成转换矩阵TTT，就可以使得降维后的矩阵YYY的维度与维度之间的差异值最大。 关于代码和计算，matlab中有princomp和pca函数可以直接计算。 这里数据用的是鸢尾花数据集， 代码如下： 123456load fixeddata; [coeff,score,latent] = pca(newdata); result=score(:,1:2); x=result(:,1); y=result(:,2); scatter(x,y,&#x27;x&#x27;); 结果如图： 综上，PCA是一种很常用的降维方法，也是一种无监督的降维方法。同时，从PCA的原理中可以看出，PCA对于线性相关的降维效果会比较好，但是对于非线性的数据，其降维效果可能就会差很多。","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankmartinem.github.io/categories/Algorithm/"}],"tags":[{"name":"PCA","slug":"PCA","permalink":"http://frankmartinem.github.io/tags/PCA/"}]},{"title":"Deep Leaning Using Matlab","slug":"Deep-Leaning-Using-Matlab","date":"2020-01-21T06:05:15.000Z","updated":"2021-04-13T01:00:37.638Z","comments":true,"path":"2020/01/21/Deep-Leaning-Using-Matlab/","link":"","permalink":"http://frankmartinem.github.io/2020/01/21/Deep-Leaning-Using-Matlab/","excerpt":"","text":"Deep Learning 常见的matlab搭建神经网络的代码结构 123options = trainingOptions(solverName, Name, Value);layer = [layer1, layer2];net = trainNetwork(TrainX, TrainY, options, layers); 其中，trainingOptions的主要作用就是设置网络中的一些参数，主要包括以下参数： 参数名称 参数值 参数含义 solverName ‘sgdm’|’rmsprop’|’adam’ 优化方法 Plots ‘none’ | ‘training-progress’ 是否画优化曲线 Verbose 1 | 0 是否显示优化信息，包括Loss，Epoch等信息 VerboseFrequency int value 多长时间刷新一次信息，默认值是50 MaxEpochs int value 最大循环次数，训练数据最多重复多少次 MiniBatchSize int value 最小的batch size，每次训练的最小数据量 ‘Shuffle’ ’once‘ | ‘never’ | ‘every-epoch’ 每个epoch是否重新排序训练数据 ‘ValidationData’ imageData，Data Store, Table，Cell array{X, Y} 用来验证网络的数据，一般用cell ‘ValidationFrequency’ int value 迭代多少次验证一次 ’ValidationPatience’ int value 如果这次的验证loss比上一次大，这种情况出现的次数超过这个值，那么停止网络训练 ‘InitialLearnRate’ scalar 初始学习率 ‘LearnRateSchedule’ ‘none’|’piecewise’ 调整学习率下降的方法，’piecewise‘方法会隔特定数目的epoch(LearnRateDropPeriod)就将LearnRate乘以一个factor(LearRateDropFactor) ’LearnRateDropPeriod’ int value 隔特定数目的epoch调整一次LearnRate ‘LearnRateDropFactor’ scalar(0-1) 每次LearnRate调整时乘以的因子 ‘L2Regularization’ nonnegative scalar 用来减少过拟合，（需要继续学习） ’Momentum’ scalar(0-1) 动量，sgdm中前一次迭代中的参数在下一次迭代中所占的比例 ‘GradientDecayFactor’ scalar(0-1) adam方法中梯度值降低的平均值 ’SquaredGradientDecayFactor‘ nonnegative scalar less than 1 梯度平方降低的平均值（Adam， RMSProp） ’Epsilon‘ int value 分母的偏置值(Adam, RMSProp) ‘ResetInputNormalization’ true | false 每次训练都将输入值标准化 ’GradientThreshold‘ int value 梯度的阈值 ’GradientThresholdMethod‘ ’l2norm‘|’global-l2norm’|’absolute-value’ 梯度阈值的计算方法 ’SequenceLength‘ ‘longest’|’shortest’|int value 输入的序列长度 ’SequencePaddingDirection‘ ’right’|’left’ 如果序列需要截取，截取的方向 ‘SequencePaddingValue’ int value 填充到序列中的值，用来补充数据长度 ‘ExecutionEnvironment’ ‘auto’|’cpu’|’gpu’|’multi-gpu’|’parallel’ 选择硬件 ‘WorkLoad’ scalar(0-1)|int|vector GPU或CPU的负载，用到的核心数，以及并行计算时的负载 ‘DispatchInBackground’ false | true 后台拆分数据并分配核心同时读取 ‘CheckpointPath’ character 存放网络训练中间值的路径 ‘OutputFunc’ function handle 网络训练时，trainNetwork函数会在刚开始训练时，每次迭代结束时，训练结束时调用这个函数 trainingOptions函数主要用来设置网络训练过程中的参数，需要熟悉其中的参数的作用 layers主要表示网络的结构，层与层之间的连接等，其中主要包括各种网络层，目前常用的有如下几类： 网络层 参数 含义 allLayer Name 网络层的名称 NumInputs 输入的个数 InputNames 输入数据的名称，cell NumOutputs 输出的个数 OutputNames 输出数据的名称，cell sequenceLayer InputSize 输入序列的大小，如果是图像的话，那么就是一个三维或四维矩阵 Normalization 数据归一化的方法选择 NormalizationDimension 归一化的维度，按照通道，按照元素或者全部统一 Mean 设置数据的均值配合zscore和zerocenter等归一化方法使用 StandardDeviation 标准差，配合对应的归一化方法使用 Min 归一化后的最小值 Max 归一化后的最大值 fullyConnectedLayer OutputSize 输出层大小 InputSize 输入层大小","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankmartinem.github.io/categories/Deep-Learning/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://frankmartinem.github.io/tags/RNN/"}]},{"title":"C Sharp Notes","slug":"C-Sharp-Notes","date":"2020-01-21T06:05:01.000Z","updated":"2021-04-13T01:00:01.932Z","comments":true,"path":"2020/01/21/C-Sharp-Notes/","link":"","permalink":"http://frankmartinem.github.io/2020/01/21/C-Sharp-Notes/","excerpt":"","text":"C#语法相关 Lambda表达式 委托的另一种表达方式 12delegate NumberChange(int Funcn);NumberChange nc1 = (Funcn) =&gt; Funcn + 10; // =&gt;读作 goes to 委托(delegate) 委托有点类似C++中的函数指针，其参数可以是一个函数，例如: 1deletate int NumberChange(int Funcn); 定义中，Func是一个返回int类型的函数 委托的多播(multicasting) 多个相同类型的委托可以合并，例如： 12345delegate int NumberChange(int Funcn);NumberChange nc;NumberChange nc1 = new NumberChange(AddNum); //AddNum是一个函数，返回intNumberChange nc2 = new NumberChange(MultiNum); // MultiNum是一个函数，返回intnc = nc1 + nc2; 委托的实例化不带有任何参数 匿名委托 123456delegate int NumberChange(int FUncn);NumberChange nc1 = delegate(int Funcn);&#123; //实现AddNum的功能 return Funcn + 10;&#125; 泛型委托 委托函数参数类型有多种，但是返回参数类型是最后一个 123456789Func&lt;int, int, bool&gt; gwl = (p, j) =&gt;&#123; if(p+j==10) return true; else return false;&#125;Console.WriteLine(gwl(5, 5) + &quot;&quot;);Console.ReadKey(); sealed关键字 类似于Java中的final关键字，sealed修饰的类不能被继承 ​ list 的用法 泛型list，和泛型委托的概念类似，其定义为： 1234567List&lt;T&gt; testList = new List&lt;T&gt;(IEnumerable&lt;T&gt; Collection);// e.gstring[] temArr = &#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;&#125;;List&lt;string&gt; testList = new List&lt;string&gt;(tempArr);//或者这么写List&lt;string&gt; testList = new List&lt;string&gt;&#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;&#125;; 和C++中的vector动态数组有点类似 list的主要函数也和vector比较类似，例如： 123456789101112List.Add(T item);//添加单个元素List.AddRange(IEnumerable&lt;T&gt; collection);//添加一组元素List.Insert(int index, T item); //在index处添加元素itemList.Remove(T item);//移除元素itemList.RemveAt(int index);//移除index处的元素List.RemoveRange(int index, int count);//移除index处开始的count个元素List.Contains(T item);//判断是否包含元素itemList.Sort();//List排序List.Reverse();//翻转ListList.Clear();//清除ListList.Count();//计算List中元素的个数List.Find(Predicate&lt;T&gt; match);//搜索List中满足条件的元素，并返回第一个元素","categories":[{"name":"coding","slug":"coding","permalink":"http://frankmartinem.github.io/categories/coding/"}],"tags":[{"name":"C#","slug":"C","permalink":"http://frankmartinem.github.io/tags/C/"},{"name":"Unity","slug":"Unity","permalink":"http://frankmartinem.github.io/tags/Unity/"}]},{"title":"Kalman","slug":"Kalman","date":"2019-11-03T07:46:44.000Z","updated":"2021-04-13T01:01:22.845Z","comments":true,"path":"2019/11/03/Kalman/","link":"","permalink":"http://frankmartinem.github.io/2019/11/03/Kalman/","excerpt":"","text":"Kalman Filter本文简单介绍了卡尔曼滤波(Kalman Filter)的基本原理以及我对卡尔曼滤波的一些理解。 首先谈一下我的一点点理解卡尔曼滤波是目前应用很广泛的一种滤波方法，最早由Kalman老先生在1960年提出，网上可以找到原文。这种方法最开始用在航天领域，作为轨道矫正的一种方法，有很好的效果。 卡尔曼滤波的方法的核心思想，就是用另一个测量空间的观测值去纠正当前空间对被测量的量的估计。简单来说，就是用一种方法去测量一个量。同时建立一个模型去估计这个测量的量，最后，按权重的方式求这两种方式的和，就是滤波之后的量的值。而这个权重的大小，就是卡尔曼系数。 公式推导首先，我们假设要测量的量为$x$, 这个量有一个模型去描述其随时间的变化，例如计算每天的温度变化，可以大致根据之前几天的温度变化规律得到一个计算矩阵，这里也有一个计算模型去计算这个变量$x$ $$x_t=Fx_{t-1} + w_t$$ $$w_{t} - N(0,Q)$$ 其中$F$为转换矩阵，$w_{t-1}$表示$t-1$时刻的噪声，且该噪声服从高斯分布。在其他的卡尔曼滤波公式推导中，会有一个额外的控制量，这里不考虑这个量。 对于测量矩阵，也有一个公式去转换。例如测量温度可以用温度传感器来测量，但是温度传感器的测量是因为温度改变了电阻的阻值，所以根据电压电流以及电阻随温度的变化曲线而计算出来的。在卡尔曼模型中，这一公式可以表示为如下等式 $$z_t=Hx_t+v_t$$ $v_t - N(0,R)$ 其中，$z_t$是通过测量的量，对应到上述的例子中，就是温度传感器的电阻阻值，$x_t$就是温度。$H$是测量矩阵，用来将测量的量转换成要估计的量。$v_t$是测量过程中存在的误差。同样的，$v_t$也是服从高斯分布的白噪声。 然后就是卡尔曼滤波的核心思想了，有了这两种方法得到的$x_t$，那么怎么得到一个更准确的估计值。所以需要将两种方法得到的估计值进行算一下加权平均，就得到了最优的估计值。所以卡尔曼滤波的方法如下： 首先根据模型计算当前时刻的估计值$$x_t’=Fx_{t-1} + w_t$$ 然后根据测量矩阵计算当前的测量值的估计值 $$z_t’=Hx_t’+v_t$$ 然后计算测量值和测量估计值之间的差，并以此作为对最终估计值的调整。从这里可以看出，如果$x_t’$估计的很准，就是说此时$z_t$的值和$z_t’$的值相差很小，那么$z_t$对于$x_t$的修正也就越少。但是如果估计值和测量值相差很大，那么$z_t$对$x_t$的修正也就越大。其中，$K_t$是卡尔曼增益，表示滤波器对测量值的信任程度。 $$x_t=x_t’+K_t*(z_t-z_t’)$$ 那么如何估计卡尔曼增益，可以用贝叶斯估计的方法推导，也可以用最小二乘法的方式推导，这里用最小二乘法的方式推导 我们假设真实值是$X_t$，那么卡尔曼滤波计算得到的估计值和真实值之间的协方差 $$P(x_t|X_t)= E[(X_t-x_t)(X_t-x_t)^T]$$ 卡尔曼滤波的估计值和模型的估计值之间的协方差，用来评估两种估计的差别 $$P(x|x’)=E[(x_t-x_t’)(x_t-x_t’)^T]$$ 根据卡尔曼的估计公式以及测量公式，可以得到 $$P(x_t|X_t)=E [(X_t - x_t’ - K_t * ( z_t - z_t’)) ( X_t - x_t’ - K_t * ( z_t - z_t’ ))^T]$$ $$=E[((I-K_tH)(X_t-x_t’)-K_tv_t)((I-K_tH)(X_t-x_t’)-K_tv_t)^T]$$ 把上述等式展开，可以得到$$P(x_t|X_t)=(I-K_tH)P(x_t|x_t’)(I-K_tH)+K_tE[v_tv_t^T]K_t^T$$ $$=P(x_t|x_t’)-K_tHP(x_t|x_t’)-P(x_t|x_t’)H^TK_t^T+K_t(HP(x_t|x_t’)H^T+R)K_t^T$$ 所以，如果我们要估计的更准确，那么就要$P(x_t|X_t)$更小，就是说真实值和卡尔曼滤波的估计值之间的协方差最小。不考虑估计值之间的相关，那么协方差矩阵的对角线元素就表示了卡尔曼估计值和真实值之间的方差。接下来就是求方差最小的情况下对应的卡尔曼增益$K_t$。可以用矩阵的迹的方法求解$$tr(P(x_t|X_t)) = tr(P(x_t|x_t’))-2tr(K_tHP(x_t|x_t’))+tr(K_t(HP(x_t|x_t’)H^T+R)K_t^T)$$ 可以看出，$tr(P(x_t|X_t))$是$K_t$的二次函数，所以根据二次函数求极值的方法，对tr(P(x_t|X_t))求导，得到 $$\\frac{d(tr(P(x_t|X_t)))}{d(K_t)}=-2(HP(x_t|x_t’))^T+2K_t(HP(x_t|x_t’)H^T+R)$$ 令$\\frac{d(tr(P(x_t|X_t)))}{d(K_t)}=0$，所以有 $$K_t=P(x_t|x_t’)H^T(HP(x_t|x_t’)H^T+R)^{-1}$$ 把$K_t$的结果带入到$P(x_t|X_t)$的表达式中，有 $$P(x_t|X_t)=P(x_t|x_t’)-K_tHP(x_t|x_t’)-P(x_t|x_t’)H^TK_t^T+K_t(HP(x_t|x_t’)H^T+R)K_t^T$$ $$=P(x_t|x_t’)-K_tHP(x_t|x_t’)-\\frac{HP(x_t|x_t’)^TP(x_t|x_t’)H^T}{HP(x_t|x_t’)H^T+R}+\\frac{HP(x_t|x_t’)^TP(x_t|x_t’)H^T}{HP(x_t|x_t’)H^T+R}$$ $$=P(x_t|x_t’)-K_tHP(x_t|x_t’)=(I-K_tH)P(x_t|x_t’)$$ 所以根据上述的推导计算，可以得到卡尔曼滤波的计算过程： 首先，根据已知的模型，以及上一时刻的卡尔曼估计值，计算当前时刻的模型预测值 $$x_t’=Fx_{t-1}$$ 根据当前的模型预测值，计算对应的协方差 $$P(x_t|x_t’)=FP(x_t|X_t)F^T$$ 根据当前的协方差和测量空间的转换矩阵，计算当前时刻的卡尔曼增益 $$K_t=P(x_t|x_t’)H^T(HP(x_t|x_t’)H^T+R)^{-1}$$ 根据卡尔曼增益和测量值，计算当前时刻的卡尔曼估计值 $$x_t=x_t’+K_t(z_t-Hx_t’)$$ 计算了当前时刻的卡尔曼估计值之后，还需要计算当前的估计值和真实值的协方差矩阵，方便下一次计算 $$P(x_t|X_t)=(I-HK_t)P(x_t|x_t’)$$ 以上就是卡尔曼滤波的基本过程，以及一些简单的推导。总体上说理解卡尔曼滤波应该算一种最优估计的算法。也是应用很广泛的，然后卡尔曼滤波的推导方法也有很多，除了最小二乘法，也可以从贝叶斯估计的角度推导。两者是类似的。 Code","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankmartinem.github.io/categories/Algorithm/"}],"tags":[{"name":"Kalman","slug":"Kalman","permalink":"http://frankmartinem.github.io/tags/Kalman/"}]}],"categories":[{"name":"coding","slug":"coding","permalink":"http://frankmartinem.github.io/categories/coding/"},{"name":"powershell","slug":"powershell","permalink":"http://frankmartinem.github.io/categories/powershell/"},{"name":"usage","slug":"usage","permalink":"http://frankmartinem.github.io/categories/usage/"},{"name":"configuration","slug":"configuration","permalink":"http://frankmartinem.github.io/categories/configuration/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankmartinem.github.io/categories/Algorithm/"},{"name":"software","slug":"software","permalink":"http://frankmartinem.github.io/categories/software/"},{"name":"Lecture","slug":"Lecture","permalink":"http://frankmartinem.github.io/categories/Lecture/"},{"name":"Mathematics","slug":"Mathematics","permalink":"http://frankmartinem.github.io/categories/Mathematics/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankmartinem.github.io/categories/Deep-Learning/"}],"tags":[{"name":"software","slug":"software","permalink":"http://frankmartinem.github.io/tags/software/"},{"name":"C++","slug":"C","permalink":"http://frankmartinem.github.io/tags/C/"},{"name":"Linux","slug":"Linux","permalink":"http://frankmartinem.github.io/tags/Linux/"},{"name":"Optimal Linear Estimation","slug":"Optimal-Linear-Estimation","permalink":"http://frankmartinem.github.io/tags/Optimal-Linear-Estimation/"},{"name":"software usage","slug":"software-usage","permalink":"http://frankmartinem.github.io/tags/software-usage/"},{"name":"Lecture","slug":"Lecture","permalink":"http://frankmartinem.github.io/tags/Lecture/"},{"name":"Computer Science","slug":"Computer-Science","permalink":"http://frankmartinem.github.io/tags/Computer-Science/"},{"name":"Linear Regression","slug":"Linear-Regression","permalink":"http://frankmartinem.github.io/tags/Linear-Regression/"},{"name":"Wiener Filter","slug":"Wiener-Filter","permalink":"http://frankmartinem.github.io/tags/Wiener-Filter/"},{"name":"python","slug":"python","permalink":"http://frankmartinem.github.io/tags/python/"},{"name":"Linear Algebra","slug":"Linear-Algebra","permalink":"http://frankmartinem.github.io/tags/Linear-Algebra/"},{"name":"Gradient Descent","slug":"Gradient-Descent","permalink":"http://frankmartinem.github.io/tags/Gradient-Descent/"},{"name":"Unscented Kalman Filter","slug":"Unscented-Kalman-Filter","permalink":"http://frankmartinem.github.io/tags/Unscented-Kalman-Filter/"},{"name":"Unity","slug":"Unity","permalink":"http://frankmartinem.github.io/tags/Unity/"},{"name":"Statistics","slug":"Statistics","permalink":"http://frankmartinem.github.io/tags/Statistics/"},{"name":"PVA","slug":"PVA","permalink":"http://frankmartinem.github.io/tags/PVA/"},{"name":"Hidden Markov Model","slug":"Hidden-Markov-Model","permalink":"http://frankmartinem.github.io/tags/Hidden-Markov-Model/"},{"name":"PCA","slug":"PCA","permalink":"http://frankmartinem.github.io/tags/PCA/"},{"name":"RNN","slug":"RNN","permalink":"http://frankmartinem.github.io/tags/RNN/"},{"name":"C#","slug":"C","permalink":"http://frankmartinem.github.io/tags/C/"},{"name":"Kalman","slug":"Kalman","permalink":"http://frankmartinem.github.io/tags/Kalman/"}]}